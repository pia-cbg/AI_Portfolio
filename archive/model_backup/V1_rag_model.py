import os
import sys
import json
from typing import Dict, List
from datetime import datetime
import openai
from dotenv import load_dotenv, find_dotenv

# .env íŒŒì¼ì„ ìƒìœ„ í´ë”ì—ì„œ ìë™ íƒìƒ‰í•´ì„œ ë¡œë“œ
load_dotenv(find_dotenv())
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from utils.music_utils import extract_musical_terms

# system prompt import
from src.prompts.prompts import GROUNDING_SYSTEM_PROMPT

class RAGModel:
    def __init__(self, retriever, model_name: str = DEFAULT_MODEL, min_similarity_score: float = 0.7):
        self.retriever = retriever
        self.model_name = model_name
        self.min_similarity_score = min_similarity_score
        self.gap_logs = []  # gap ì¼€ì´ìŠ¤ ê¸°ë¡
        self.stats = {
            'total_queries': 0,
            'response_errors': 0,
            'gap_cases': 0
        }
        self.client = openai.OpenAI(api_key=OPENAI_API_KEY)

    def get_conversation_response(self, query: str) -> Dict:
        """ëª¨ë“  ë‹µë³€ì„ LLMì´ reasoning. í•„ìš”ì‹œ gap ë¡œê·¸ë„ ë‚¨ê¹€."""
        self.stats['total_queries'] += 1
        musical_terms = extract_musical_terms(query)

        try:
            sources = self.retriever.search(query, top_k=2) if self.retriever else []

            # gap(ìë£Œ ì—†ìŒ) ì—¬ë¶€ ê¸°ë¡
            is_gap = len(sources) == 0 or all(s.get("score", 0) < self.min_similarity_score for s in sources)
            if is_gap:
                self.stats['gap_cases'] += 1
                self._log_gap_case(query, musical_terms)

            # í•­ìƒ LLMì´ extrapolation/reasoningí•˜ê²Œ ë„˜ê¹€
            return self._generate_llm_response(query, sources, musical_terms)
        except Exception as e:
            self.stats['response_errors'] += 1
            return self._create_error_response(f"ì˜¤ë¥˜: {e}")

    def _generate_llm_response(self, query: str, sources: List[Dict], musical_terms: List[str]) -> Dict:
        """í•­ìƒ LLMì´ ìë£Œì¶©ë¶„/ë¶ˆì¼ì¹˜/ë¶€ì¡±/ìë£Œ ì—†ìŒ ë“± ëª¨ë‘ reasoningí•˜ê²Œ ìœ ë„."""
        user_content = self._format_user_message(query, sources)
        try:
            chat = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": GROUNDING_SYSTEM_PROMPT},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            answer = chat.choices[0].message.content.strip()
            return {
                'answer': answer,
                'sources': sources,
                'model': self.model_name,
                'musical_terms': musical_terms,
                'timestamp': datetime.now().isoformat(),
                'used_grounding_prompt': True
            }
        except Exception as e:
            self.stats['response_errors'] += 1
            return self._create_error_response(f"API ì˜¤ë¥˜: {e}")

    def _format_sources_for_prompt(self, sources: List[Dict]) -> str:
        """
        ì—¬ëŸ¬ passageë¥¼ í”„ë¡¬í”„íŠ¸ìš© blockìœ¼ë¡œ í¬ë§·.
        - passage ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í¬í•¨
        - ê° passageëŠ” 200ì(ë˜ëŠ” ì›í•˜ì‹œëŠ” ìë¦¿ìˆ˜) ì´ë‚´ë¡œ ì˜ë¼ì¤Œ
        - ë¶ˆí•„ìš”í•œ íƒ€ì´í‹€/ì ìˆ˜ ë“±ì€ ìƒëµ, ì§„ì§œ ë‚´ìš©ë§Œ
        """
        if not sources:
            return ""
        formatted = ""
        max_passage = 2    # ì°¸ê³ ìë£Œ ìµœëŒ€ ê°œìˆ˜
        max_length = 150   # passage ë³„ ìµœëŒ€ ê¸¸ì´(ì ìˆ˜)
        for idx, source in enumerate(sources[:max_passage], 1):
            content = source.get('content', 'ë‚´ìš© ì—†ìŒ').strip()
            # ë³¸ë¬¸ ê¸¸ì´ ì œí•œ
            if len(content) > max_length:
                content = content[:max_length] + "..."
            # block format (title, score ë“± ìƒëµ)
            formatted += f"\n[ì°¸ê³ ìë£Œ {idx}]\në‚´ìš©: {content}\n"
            formatted += "-" * 32
        return formatted

    def _format_user_message(self, query: str, sources: List[Dict]) -> str:
        """ì§ˆë¬¸ + ì°¸ê³  passageë¥¼ ë¬¶ì–´ user í”„ë¡¬í”„íŠ¸í™”"""
        sources_text = self._format_sources_for_prompt(sources)
        if sources_text.strip():
            return f"{query}\n\nì°¸ê³ ìë£Œ:\n{sources_text}"
        else:
            return query

    def _log_gap_case(self, query: str, musical_terms: List[str]):
        """gap(ê·¼ê±° ì—†ìŒ/ë¶ˆì¶©ë¶„) ìƒí™© ê¸°ë¡(í†µê³„, DB ë³´ê°• ìš©ë„)"""
        self.gap_logs.append({
            "query": query,
            "musical_terms": musical_terms,
            "timestamp": datetime.now().isoformat()
        })

    def save_gap_report(self, filename: str = None):
        """gap ì¼€ì´ìŠ¤ ë¦¬í¬íŠ¸ ì €ì¥(ë°ì´í„°ì…‹ ë³´ê°•/ìš´ì˜ì§„ í”¼ë“œë°± ìš©)"""
        if not self.gap_logs:
            print("gap ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'data/fine_tuning/gaps/gap_report_{timestamp}.json'

        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.gap_logs, f, ensure_ascii=False, indent=2)
        print(f"âœ… gap ë¦¬í¬íŠ¸ ì €ì¥: {filename} (ì´ {len(self.gap_logs)}ê±´)")

    def _create_error_response(self, error_message: str) -> Dict:
        return {
            'answer': f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {error_message}",
            'sources': [],
            'model': self.model_name,
            'musical_terms': [],
            'confidence': 'error',
            'data_coverage': 'error'
        }

    def get_session_stats(self) -> Dict:
        return {
            'statistics': self.stats,
            'gaps_logged': len(self.gap_logs)
        }


def main():
    try:
        from src.models.backup.V1_retriever import VectorRetriever
        retriever = VectorRetriever()

        print("ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        retriever.load_embeddings()
        retriever.build_index()

        rag_model = RAGModel(retriever)

        # ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_queries = [
            "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ë€?",
            "12 equal temperamentì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
            "í‰ê· ìœ¨ê³¼ ìˆœì •ë¥ ì˜ ì°¨ì´ëŠ”?",
            "Abm7(b5)ëŠ” ì–´ë–»ê²Œ í‘œê¸°í•˜ëŠ”ê±°ì•¼?"  # ì¼ë¶€ëŸ¬ ìë£Œ ì—†ì„ë²•í•œ ì§ˆì˜
        ]

        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸: {query}")
            print('='*60)

            response = rag_model.get_conversation_response(query)

            print("\n[ë‹µë³€]")
            print(response['answer'])
            print(f"\n[ì°¸ê³  passage ê°œìˆ˜]: {len(response['sources'])}")
            print(f"[ëª¨ë¸]: {response['model']}")
            print(f"[íƒ€ì„ìŠ¤íƒ¬í”„]: {response['timestamp']}")

        # ì„¸ì…˜ í†µê³„ ë° gap ë¦¬í¬íŠ¸ ì €ì¥
        print("\nğŸ“Š ì„¸ì…˜ í†µê³„ ë° gap ë¡œê·¸:")
        stats = rag_model.get_session_stats()
        print(json.dumps(stats, indent=2))
        rag_model.save_gap_report()

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()