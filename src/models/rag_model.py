import os
import sys
import json
from typing import Dict, List
from datetime import datetime
import openai
from dotenv import load_dotenv, find_dotenv

# .env íŒŒì¼ì„ ìƒìœ„ í´ë”ì—ì„œ ìë™ íƒìƒ‰í•´ì„œ ë¡œë“œ
load_dotenv(find_dotenv())
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

from utils.music_utils import extract_musical_terms
from src.prompts.prompts import GROUNDING_SYSTEM_PROMPT

class RAGModel:
    def __init__(self, retriever, model_name: str = DEFAULT_MODEL, min_similarity_score: float = 0.7):
        self.retriever = retriever
        self.model_name = model_name
        self.min_similarity_score = min_similarity_score
        # self.gap_logs = []  # gap ì¼€ì´ìŠ¤ ê¸°ë¡ (gap ê¸°ëŠ¥ ì œê±°)
        # self.stats = {
        #     'total_queries': 0,
        #     'response_errors': 0,
        #     'gap_cases': 0
        # }
        self.client = openai.OpenAI(api_key=OPENAI_API_KEY)

    def get_conversation_response(self, query: str) -> Dict:
        """LLMì´ reasoning, í•„ìš”ì‹œ errorë§Œ ë¦¬í„´(gap ê¸°ëŠ¥ ì œê±°)."""
        # self.stats['total_queries'] += 1
        musical_terms = extract_musical_terms(query)

        try:
            sources = self.retriever.search(query, top_k=2) if self.retriever else []

            # gap(ìë£Œ ì—†ìŒ) ê´€ë ¨ ì½”ë“œ ì™„ì „ ì‚­ì œ
            # is_gap = len(sources) == 0 or all(s.get("score", 0) < self.min_similarity_score for s in sources)
            # if is_gap:
            #     self.stats['gap_cases'] += 1
            #     self._log_gap_case(query, musical_terms)

            return self._generate_llm_response(query, sources, musical_terms)
        except Exception as e:
            # self.stats['response_errors'] += 1
            return self._create_error_response(f"ì˜¤ë¥˜: {e}")

    def _generate_llm_response(self, query: str, sources: List[Dict], musical_terms: List[str]) -> Dict:
        user_content = self._format_user_message(query, sources)
        try:
            chat = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": GROUNDING_SYSTEM_PROMPT},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            answer = chat.choices[0].message.content.strip()
            return {
                'answer': answer,
                'sources': sources,
                'model': self.model_name,
                'musical_terms': musical_terms,
                'timestamp': datetime.now().isoformat(),
                'used_grounding_prompt': True
            }
        except Exception as e:
            # self.stats['response_errors'] += 1
            return self._create_error_response(f"API ì˜¤ë¥˜: {e}")

    def _format_sources_for_prompt(self, sources: List[Dict]) -> str:
        if not sources:
            return ""
        formatted = ""
        max_passage = 2
        max_length = 300     # ì •ì˜, ì›ë¦¬, ì˜ˆì‹œê¹Œì§€ ê°ë‹¹í•  ìˆ˜ ìˆê²Œ ë” í™•ëŒ€
        for idx, source in enumerate(sources[:max_passage], 1):
            concept_ko = source.get('concept.ko', '')
            concept_en = source.get('concept.en', '')
            aliases = source.get('aliases', '')
            definition = source.get('definition', '')
            logic = source.get('logic', '')
            example_name = source.get('examples.name', '')
            example_desc = source.get('examples.description', '')
            tips = source.get('tips', '')

            definition = (definition[:max_length] + "...") if len(definition) > max_length else definition
            logic = (logic[:max_length] + "...") if len(logic) > max_length else logic

            formatted += (
                f"\n[ì°¸ê³ ìë£Œ {idx}]\n"
                f"ìš©ì–´(í•œê¸€): {concept_ko}\n"
                f"ìš©ì–´(ì˜ë¬¸): {concept_en}\n"
                f"ë™ì˜ì–´Â·ìœ ì‚¬ í‘œê¸°: {aliases}\n"
                f"[ì •ì˜]: {definition}\n"
                f"[ì›ë¦¬]: {logic}\n"
            )
            if example_name:
                formatted += f"ì˜ˆì‹œ: {example_name}\n"
                if example_desc:
                    formatted += f"ì˜ˆì‹œ ì„¤ëª…: {example_desc}\n"
            if tips:
                formatted += f"[íŒ]: {tips}\n"
            formatted += "-"*28
        return formatted

    def _format_user_message(self, query: str, sources: List[Dict]) -> str:
        sources_text = self._format_sources_for_prompt(sources)
        # ë””ë²„ê¹…ìš© print ëª¨ë‘ ì œê±° (ì‹¤ì„œë¹„ìŠ¤ì—ì„  ë¹„ì¶”)
        if sources_text.strip():
            return (
                f"ì§ˆë¬¸: {query}\n\n"
                "ì•„ë˜ ì°¸ê³ ìë£Œì˜ [ì •ì˜]ì™€ [ì›ë¦¬]ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ë‹µí•˜ì„¸ìš”. "
                "ì°¸ê³ ìë£Œì— ì—†ëŠ” ìš©ì–´ë‚˜ ì˜ëª»ëœ ì •ë³´ë¥¼ ì„ì˜ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. "
                "ì •ì˜/ì›ë¦¬ê°€ ì œëŒ€ë¡œ ì—†ëŠ” ê²½ìš°ì—ë§Œ 'ìë£Œ ë¶€ì¡±'ì„ì„ ë°í˜€ì£¼ì„¸ìš”.\n"
                f"{sources_text}"
            )
        else:
            return (
                f"ì§ˆë¬¸: {query}\n"
                "ì°¸ê³ ìë£Œê°€ ì—†ì–´ ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

    # gap ê´€ë ¨ ëª¨ë“  í•¨ìˆ˜/ë©”ì†Œë“œ ì™„ì „ ì œê±°(ì£¼ì„)
    # def _log_gap_case(self, query: str, musical_terms: List[str]):
    #     pass
    # def save_gap_report(self, filename: str = None):
    #     pass
    # def get_session_stats(self) -> Dict:
    #     return {}

    def _create_error_response(self, error_message: str) -> Dict:
        return {
            'answer': f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {error_message}",
            'sources': [],
            'model': self.model_name,
            'musical_terms': [],
            'confidence': 'error',
            'data_coverage': 'error'
        }

def main():
    try:
        from src.models.retriever import VectorRetriever
        retriever = VectorRetriever()

        print("ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        retriever.load_embeddings()
        retriever.build_index()

        rag_model = RAGModel(retriever)
        while True:
            query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
            if query.lower() in ["exit", "quit"]:
                break
            response = rag_model.get_conversation_response(query)
            print("\n[ë‹µë³€]")
            print(response['answer'])
            print(f"\n[ì°¸ê³  passage ê°œìˆ˜]: {len(response['sources'])}")
        # # ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        # test_queries = [
        #     "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜"
        # ]

        # for query in test_queries:
        #     print(f"\n{'='*60}")
        #     print(f"ì§ˆë¬¸: {query}")
        #     print('='*60)

        #     response = rag_model.get_conversation_response(query)

        #     print("\n[ë‹µë³€]")
        #     print(response['answer'])
        #     print(f"\n[ì°¸ê³  passage ê°œìˆ˜]: {len(response['sources'])}")
        #     print(f"[ëª¨ë¸]: {response['model']}")
        #     print(f"[íƒ€ì„ìŠ¤íƒ¬í”„]: {response['timestamp']}")

        # # ì„¸ì…˜ í†µê³„ ë° gap ë¦¬í¬íŠ¸ ì €ì¥
        # print("\nğŸ“Š ì„¸ì…˜ í†µê³„ ë° gap ë¡œê·¸:")
        # stats = rag_model.get_session_stats()
        # print(json.dumps(stats, indent=2))
        # rag_model.save_gap_report()

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    pass

if __name__ == "__main__":
    main()