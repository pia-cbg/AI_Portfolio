import os
import sys
import json
from typing import Dict, List
from datetime import datetime
from dotenv import load_dotenv
import openai

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë° .env ì„¤ì •
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)
env_path = os.path.join(project_root, '.env')
load_dotenv(env_path)

openai.api_key = os.getenv("OPENAI_API_KEY")
DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

from utils.music_utils import extract_musical_terms, format_chord_name

class RAGModel:
    def __init__(self, retriever, model_name: str = DEFAULT_MODEL, min_similarity_score: float = 0.7):
        self.retriever = retriever
        self.model_name = model_name
        self.min_similarity_score = min_similarity_score
        self.session_gaps = []
        self.stats = {
            'total_queries': 0,
            'successful_answers': 0,
            'partial_answers': 0,
            'no_data_answers': 0
        }

    def get_conversation_response(self, query: str) -> Dict:
        self.stats['total_queries'] += 1
        musical_terms = extract_musical_terms(query)

        try:
            sources = self.retriever.search(query, top_k=5) if self.retriever else []

            high_quality = [s for s in sources if s.get('score', 0) >= self.min_similarity_score]
            medium_quality = [s for s in sources if 0.5 <= s.get('score', 0) < self.min_similarity_score]

            if high_quality:
                return self._generate_complete_response(query, high_quality, musical_terms)
            elif medium_quality:
                return self._generate_partial_response(query, medium_quality, musical_terms)
            else:
                return self._generate_no_data_response(query, musical_terms)

        except Exception as e:
            return self._create_error_response(f"ì˜¤ë¥˜: {e}")
        
    def _generate_complete_response(self, query: str, sources: List[Dict], musical_terms: List[str]) -> Dict:
        """ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œ ì‘ë‹µ ìƒì„±"""
        sources_text = self._format_sources_for_prompt(sources)

        prompt = f"""
ë‹¹ì‹ ì€ ìŒì•… ì´ë¡  êµìœ¡ ì‹œìŠ¤í…œì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì°¸ê³ ìë£Œ:
{sources_text}

ìœ„ ì°¸ê³ ìë£Œë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ê° ì •ë³´ë§ˆë‹¤ [ì°¸ê³ ìë£Œ ë²ˆí˜¸]ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
ì°¸ê³ ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        """

        try:
            response = openai.ChatCompletion.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an AI assistant."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )

            answer = response.choices[0].message.content.strip()

            self.stats['successful_answers'] += 1

            return {
                'answer': answer,
                'sources': sources,
                'model': self.model_name,
                'musical_terms': musical_terms,
                'confidence': 'high',
                'data_coverage': 'complete'
            }
        except Exception as e:
            return self._create_error_response(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    def _generate_partial_response(self, query: str, sources: List[Dict], musical_terms: List[str]) -> Dict:
        """ë¶€ë¶„ì  ë°ì´í„°ê°€ ìˆì„ ë•Œ ì‘ë‹µ ìƒì„±"""
        answer = "ì°¸ê³ ìë£Œì— ì¼ë¶€ ê´€ë ¨ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.\n"
        answer += self._format_sources_for_prompt(sources)
        self.stats['partial_answers'] += 1

        return {
            'answer': answer,
            'sources': sources,
            'model': self.model_name,
            'musical_terms': musical_terms,
            'confidence': 'medium',
            'data_coverage': 'partial'
        }

    def _generate_no_data_response(self, query: str, musical_terms: List[str]) -> Dict:
        """ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì‘ë‹µ ìƒì„±"""
        gap = {
            'query': query,
            'type': 'no_coverage',
            'musical_terms': musical_terms,
            'timestamp': datetime.now().isoformat()
        }
        self.session_gaps.append(gap)
        self.stats['no_data_answers'] += 1

        answer = f"""
ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°ì…‹ì— "{query}"ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.

ğŸ” ê°ì§€ëœ ìŒì•… ìš©ì–´: {', '.join(musical_terms) if musical_terms else 'ì—†ìŒ'}

ì´ ì£¼ì œëŠ” í–¥í›„ ë°ì´í„°ì…‹ í™•ì¥ ì‹œ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤.
ë‹¤ë¥¸ ìŒì•… ì´ë¡  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.
        """

        return {
            'answer': answer,
            'sources': [],
            'model': self.model_name,
            'musical_terms': musical_terms,
            'confidence': 'none',
            'data_coverage': 'none',
            'gap_recorded': True
        }

    def _format_sources_for_prompt(self, sources: List[Dict]) -> str:
        """í”„ë¡¬í”„íŠ¸ìš© ì†ŒìŠ¤ í¬ë§·íŒ…"""
        formatted = ""
        for idx, source in enumerate(sources, 1):
            title = source.get('title', 'ì œëª© ì—†ìŒ')
            content = source.get('content', 'ë‚´ìš© ì—†ìŒ')
            score = source.get('score', 0)
            if len(content) > 500:
                content = content[:500] + "..."
            formatted += f"\n[ì°¸ê³ ìë£Œ {idx}]\n"
            formatted += f"ì œëª©: {title}\n"
            formatted += f"ë‚´ìš©: {content}\n"
            formatted += f"ê´€ë ¨ë„: {score:.3f}\n"
            formatted += "-" * 40
        return formatted

    def _create_error_response(self, error_message: str) -> Dict:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'answer': f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {error_message}",
            'sources': [],
            'model': self.model_name,
            'musical_terms': [],
            'confidence': 'error',
            'data_coverage': 'error'
        }

    def get_session_stats(self) -> Dict:
        """í˜„ì¬ ì„¸ì…˜ í†µê³„ ë°˜í™˜"""
        return {
            'statistics': self.stats,
            'gaps_identified': len(self.session_gaps),
            'gap_details': self.session_gaps
        }
        
    def save_gaps_report(self, filename: str = None):
        """ë°ì´í„° ê°­ ë¦¬í¬íŠ¸ ì €ì¥"""
        if not self.session_gaps:
            print("ê¸°ë¡ëœ ê°­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'data/fine_tuning/gaps/gap_report_{timestamp}.json'

        os.makedirs(os.path.dirname(filename), exist_ok=True)

        report = {
            'session_date': datetime.now().isoformat(),
            'statistics': self.stats,
            'total_gaps': len(self.session_gaps),
            'gaps': self.session_gaps
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ… ê°­ ë¦¬í¬íŠ¸ ì €ì¥: {filename}")
        print(f"   - ì´ ì§ˆë¬¸: {self.stats['total_queries']}")
        print(f"   - ì™„ì „ ë‹µë³€: {self.stats['successful_answers']}")
        print(f"   - ë¶€ë¶„ ë‹µë³€: {self.stats['partial_answers']}")
        print(f"   - ë‹µë³€ ë¶ˆê°€: {self.stats['no_data_answers']}")

def main():
    """RAG ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    try:
        from src.models.retriever import VectorRetriever
        retriever = VectorRetriever()

        print("ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        retriever.load_embeddings()
        retriever.build_index()

        rag_model = RAGModel(retriever)

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
        test_queries = [
            "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ë€?",
            "12 equal temperamentì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
            "í‰ê· ìœ¨ê³¼ ìˆœì •ë¥ ì˜ ì°¨ì´ëŠ”?"
        ]

        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸: {query}")
            print('='*60)

            response = rag_model.get_conversation_response(query)

            print("\në‹µë³€:")
            print(response['answer'])
            print(f"\nì‹ ë¢°ë„: {response['confidence']}")
            print(f"ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {response['data_coverage']}")

        # ì„¸ì…˜ í†µê³„ ë° ê°­ ë¦¬í¬íŠ¸
        print("\nğŸ“Š ì„¸ì…˜ í†µê³„:")
        stats = rag_model.get_session_stats()
        print(json.dumps(stats['statistics'], indent=2))

        # ê°­ ë¦¬í¬íŠ¸ ì €ì¥
        rag_model.save_gaps_report()

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()