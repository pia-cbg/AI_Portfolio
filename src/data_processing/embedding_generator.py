import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple
import pickle
import os
import torch
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.data_processing.json_loader import MusicTheoryDataLoader

class EmbeddingGenerator:
    def __init__(
        self, 
        model_name: str = None, 
        embedding_path: str = 'data/embeddings/music_theory_embeddings.pkl'
    ):
        """
        ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        
        :param model_name: ì‚¬ìš©í•  sentence-transformer ëª¨ë¸
        :param embedding_path: ì„ë² ë”© ì €ì¥ ê²½ë¡œ
        """
        # ìµœê³  ì„±ëŠ¥ ë‹¤êµ­ì–´ ëª¨ë¸ ì„ íƒ
        if model_name is None:
            model_name = "intfloat/multilingual-e5-large"
        
        print(f"ğŸµ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_name}")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(model_name, device=self.device)
        self.model_name = model_name
        
        # ì„ë² ë”© ê²½ë¡œ
        self.embedding_path = embedding_path
        
        # ì„ë² ë”© ë° ì²­í¬ ì´ˆê¸°í™”
        self.embeddings = None
        self.chunks = None
    
    def generate_embeddings(self, text_chunks: List[Dict]) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
        
        :param text_chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        :return: ì„ë² ë”© ë°°ì—´
        """
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        texts = []
        for chunk in text_chunks:
            # ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì„ë² ë”© ìƒì„±
            title = chunk.get('title', '')
            content = chunk.get('content', '')
            context = chunk.get('context', '')
            
            # ê²°í•©ëœ í…ìŠ¤íŠ¸ ìƒì„±
            combined_text = f"{title}. {context}. {content}"
            texts.append(combined_text)
        
        print(f"ğŸµ {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        # ì„ë² ë”© ì •ë³´ ì €ì¥
        self.embeddings = embeddings
        self.chunks = text_chunks
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: shape {embeddings.shape}")
        return embeddings
    
    def save_embeddings(self):
        """ì„ë² ë”© ë° ì²­í¬ ì €ì¥"""
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(self.embedding_path), exist_ok=True)
        
        embedding_data = {
            'embeddings': self.embeddings.tolist(),
            'chunks': self.chunks,
            'model_name': self.model_name
        }
        
        with open(self.embedding_path, 'wb') as f:
            pickle.dump(embedding_data, f)
        
        print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {len(self.chunks)}ê°œ, {self.embedding_path}")
    
    def load_embeddings(self) -> bool:
        """
        ì €ì¥ëœ ì„ë² ë”© ë¡œë“œ
        
        :return: ì„ë² ë”© ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with open(self.embedding_path, 'rb') as f:
                embedding_data = pickle.load(f)
            
            # ì„ë² ë”©ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
            self.embeddings = np.array(embedding_data['embeddings'])
            self.chunks = embedding_data['chunks']
            self.model_name = embedding_data.get('model_name', 'unknown')
            
            print(f"âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(self.chunks)}ê°œ, ëª¨ë¸: {self.model_name}")
            return True
        
        except FileNotFoundError:
            print(f"âŒ ì„ë² ë”© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.embedding_path}")
            return False
        
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_embeddings(self) -> Tuple[np.ndarray, List[Dict]]:
        """
        í˜„ì¬ ì„ë² ë”©ê³¼ ì²­í¬ ë°˜í™˜
        
        :return: (ì„ë² ë”© ë°°ì—´, ì²­í¬ ë¦¬ìŠ¤íŠ¸)
        """
        if self.embeddings is None or self.chunks is None:
            raise ValueError("ì„ë² ë”©ì´ ìƒì„±ë˜ê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return self.embeddings, self.chunks

    def update_embeddings(self, new_chunks: List[Dict]):
        """
        ìƒˆë¡œìš´ ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ì¶”ê°€
        
        :param new_chunks: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì²­í¬
        """
        if self.embeddings is None:
            print("ê¸°ì¡´ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            return self.generate_embeddings(new_chunks)
        
        # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì¤€ë¹„
        new_texts = []
        for chunk in new_chunks:
            title = chunk.get('title', '')
            content = chunk.get('content', '')
            context = chunk.get('context', '')
            combined_text = f"{title}. {context}. {content}"
            new_texts.append(combined_text)
        
        # ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±
        new_embeddings = self.model.encode(
            new_texts,
            batch_size=32,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        # ê¸°ì¡´ ì„ë² ë”©ê³¼ ë³‘í•©
        combined_embeddings = np.vstack([self.embeddings, new_embeddings])
        combined_chunks = self.chunks + new_chunks
        
        # ì—…ë°ì´íŠ¸
        self.embeddings = combined_embeddings
        self.chunks = combined_chunks
        
        print(f"âœ… ì„ë² ë”© ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(new_chunks)}ê°œ ì¶”ê°€")
        return combined_embeddings

    def search_similar(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
        
        :param query: ê²€ìƒ‰ ì¿¼ë¦¬
        :param top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        :return: ìœ ì‚¬í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode(
            query,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(self.embeddings, query_embedding)
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # ê²°ê³¼ ìƒì„±
        results = []
        
        for idx in top_indices:
            results.append({
                'chunk': self.chunks[idx],
                'score': float(similarities[idx])
            })
        
        return results

    def get_embedding_stats(self) -> Dict:
        """ì„ë² ë”© í†µê³„ ì •ë³´"""
        if self.embeddings is None:
            return {"status": "No embeddings loaded"}
        
        stats = {
            'model_name': self.model_name,
            'num_embeddings': len(self.embeddings),
            'embedding_dim': self.embeddings.shape[1],
            'mean_norm': float(np.mean(np.linalg.norm(self.embeddings, axis=1))),
            'std_norm': float(np.std(np.linalg.norm(self.embeddings, axis=1))),
            'memory_size_mb': self.embeddings.nbytes / (1024 * 1024)
        }
        
        return stats

def main():
    # ì‚¬ìš© ì˜ˆì‹œ
    from src.data_processing.json_loader import MusicTheoryDataLoader
    
    print("ğŸµ ìŒì•… ì´ë¡  ì„ë² ë”© ìƒì„± ì‹œì‘")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë”©...")
    loader = MusicTheoryDataLoader()
    loader.load_data()
    chunks = loader.extract_text_chunks()
    
    # 2. ì„ë² ë”© ìƒì„±
    print("\n2ï¸âƒ£ ì„ë² ë”© ìƒì„±...")
    embedder = EmbeddingGenerator()
    embeddings = embedder.generate_embeddings(chunks)
    
    # 3. í†µê³„ ì¶œë ¥
    print("\n3ï¸âƒ£ ì„ë² ë”© í†µê³„:")
    stats = embedder.get_embedding_stats()
    for key, value in stats.items():
        print(f"  - {key}: {value}")
    
    # 4. ì €ì¥
    print("\n4ï¸âƒ£ ì„ë² ë”© ì €ì¥...")
    embedder.save_embeddings()
    
    # 5. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n5ï¸âƒ£ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰...")
    test_query = "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸"
    results = embedder.search_similar(test_query, top_k=3)
    
    print(f"\nì¿¼ë¦¬: '{test_query}'")
    print("ìœ ì‚¬í•œ ì²­í¬:")
    for i, result in enumerate(results, 1):
        print(f"\n{i}. ìœ ì‚¬ë„: {result['score']:.3f}")
        print(f"   ì œëª©: {result['chunk']['title']}")
        print(f"   ë‚´ìš©: {result['chunk']['content'][:100]}...")

if __name__ == "__main__":
    main()