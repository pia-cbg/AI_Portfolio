import numpy as np
import json
import re
import os
from typing import List, Dict, Set
from sklearn.feature_extraction.text import TfidfVectorizer

class KeywordExtractor:
    def __init__(self, json_path='data/raw/music_theory_curriculum.json'):
        """
        JSON íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ì´ˆê¸°í™”
        
        :param json_path: JSON íŒŒì¼ ê²½ë¡œ
        """
        self.json_path = json_path
        self.data = self.load_json_data()
        self.keywords = set()
        
        # ìŒì•… ì´ë¡  í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
        self.music_whitelist = self._load_music_whitelist()
    
    def _load_music_whitelist(self) -> Set[str]:
        """ìŒì•… ì´ë¡  í•µì‹¬ ìš©ì–´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸"""
        return {
            # ê¸°ë³¸ ìŒì•… ìš©ì–´
            'ì½”ë“œ', 'í™”ìŒ', 'ìŠ¤ì¼€ì¼', 'ìŒê³„', 'ì¡°ì„±', 'í™”ì„±', 'ì§„í–‰', 'ìŒì •', 'ë¦¬ë“¬', 'ë©œë¡œë””', 'ë°•ì',
            
            # í™”ì„±í•™ ìš©ì–´
            'ë„ë¯¸ë„ŒíŠ¸', 'í† ë‹‰', 'ì„œë¸Œë„ë¯¸ë„ŒíŠ¸', 'í‘ì…˜', 'ê¸°ëŠ¥', 'íŠ¸ë¼ì´ì–´ë“œ', 'ì„¸ë¸ìŠ¤', 'íŠ¸ë¼ì´í†¤',
            
            # ì½”ë“œ ì¢…ë¥˜
            'ë©”ì´ì €', 'ë§ˆì´ë„ˆ', 'ë””ë¯¸ë‹ˆì‰¬ë“œ', 'ì–´ê·¸ë©˜í‹°ë“œ', 'ì„œìŠ¤íœë””ë“œ', 'ìµìŠ¤í…ë””ë“œ',
            
            # ê³ ê¸‰ í™”ì„±í•™
            'ì„¸ì»¨ë”ë¦¬ë„ë¯¸ë„ŒíŠ¸', 'ë…¼ë‹¤ì´ì•„í† ë‹‰', 'ë‹¤ì´ì•„í† ë‹‰', 'ë¦¬í•˜ëª¨ë‚˜ì´ì œì´ì…˜', 'ëª¨ë“œ', 'ëª¨ë‹¬',
            'íŠ¸ë¼ì´í†¤ì„œë¸ŒìŠ¤í‹°íŠœì…˜', 'íœíƒ€í† ë‹‰', 'ë¸”ë£¨ìŠ¤', 'í…ì…˜', 'ì–¼í„°ë ˆì´ì…˜',
            
            # ì§„í–‰/ì›€ì§ì„
            'ê°•ì§„í–‰', 'ì•½ì§„í–‰', 'ìˆœì°¨ì§„í–‰', 'ë„ì•½ì§„í–‰', 'ë³‘ì§„í–‰', 'ë°˜ì§„í–‰', 'ì‚¬ì„±ë¶€',
            
            # ìŒì•… í˜•ì‹
            'í”„ë ˆì´ì¦ˆ', 'ëª¨í‹°ë¸Œ', 'ì¼€ì´ë˜ìŠ¤', 'ì¢…ì§€', 'ì•„ë¥´í˜ì§€ì˜¤', 'ë³´ì´ì‹±', 'ì¸ë²„ì „', 'ì „ìœ„',
            
            # ë¦¬ë“¬/ë°•ì
            'ì‹±ì½”í˜ì´ì…˜', 'í—¤ë¯¸ì˜¬ë¼', 'í´ë¦¬ë¦¬ë“¬', 'ìŠ¤ìœ™', 'ì…”í”Œ', 'ê·¸ë£¨ë¸Œ',
            
            # ê¸°íƒ€ ì¤‘ìš” ìš©ì–´
            'ê°€ì´ë“œí†¤', 'í¬ë¡œë§¤í‹±', 'ë‹¤ì´ì•„í† ë‹‰', 'ì—”í•˜ëª¨ë‹‰', 'ëª¨ë“ˆë ˆì´ì…˜', 'íŠ¸ëœìŠ¤í¬ì§€ì…˜',
            'í˜ë‹¬í¬ì¸íŠ¸', 'ì˜¤ìŠ¤í‹°ë‚˜í† ', 'ì¹´ìš´í„°í¬ì¸íŠ¸', 'ëŒ€ìœ„ë²•'
        }
    
    def load_json_data(self) -> Dict:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as file:
                return json.load(file)
        except FileNotFoundError:
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.json_path}")
            return {}
        except json.JSONDecodeError:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {self.json_path}")
            return {}
    
    def extract_text_corpus(self) -> List[str]:
        """JSON ë°ì´í„°ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        corpus = []
        
        def extract_text_recursive(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    # í‚¤ ì´ë¦„ë„ ì¤‘ìš”í•œ ì •ë³´ë¡œ í¬í•¨
                    if key not in ['metadata', 'constants']:
                        corpus.append(key.replace('_', ' '))
                    
                    if isinstance(value, str) and len(value.strip()) > 5:
                        corpus.append(value.strip())
                    elif isinstance(value, (dict, list)):
                        extract_text_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_text_recursive(item)
        
        extract_text_recursive(self.data)
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(corpus)}")
        return corpus
    
    def extract_keywords_with_tfidf(self, corpus: List[str], top_n: int = 100) -> Set[str]:
        """
        TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìŒì•… ì´ë¡  íŠ¹í™”)
        
        :param corpus: í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤
        :param top_n: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        :return: ì¶”ì¶œëœ í‚¤ì›Œë“œ ì„¸íŠ¸
        """
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        def preprocess(text):
            text = re.sub(r'[^\w\s#â™­â™¯Â°]', ' ', text)
            return text.lower()
        
        # í™•ì¥ëœ ë¶ˆìš©ì–´
        korean_stopwords = [
            'ì˜', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì—', 'ì„œ', 'ë¡œ', 'ê³¼', 'ì™€', 'ë“±', 'ë°', 'ë˜í•œ', 'ë•Œë¬¸ì—', 
            'í†µí•´', 'ìœ„í•´', 'ë•Œ', 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì¤ë‹ˆë‹¤', 'ë§í•©ë‹ˆë‹¤', 'ëŠë‚Œì„', 
            'ì‚¬ìš©ë©ë‹ˆë‹¤', 'ì ìš©ë©ë‹ˆë‹¤', 'ì˜ˆë¥¼', 'ë“¤ì–´', 'ê°™ì€', 'ë‹¤ë¥¸', 'ë§¤ìš°', 'íŠ¹íˆ', 'ê°€ì¥', 'ëª¨ë“ ', 
            'ì–´ë–¤', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'ìˆëŠ”', 'ì—†ëŠ”', 'ìˆê³ ', 'ìˆìœ¼ë©°', 'ìˆì–´', 'ì—ì„œëŠ”', 'ë¡œì„œ', 'ìœ¼ë¡œ'
        ]
        english_stopwords = ['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
        all_stopwords = korean_stopwords + english_stopwords
        
        # ì½”í¼ìŠ¤ ì „ì²˜ë¦¬
        preprocessed_corpus = [preprocess(doc) for doc in corpus]
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(
            stop_words=all_stopwords,
            max_features=300,  # ì—¬ìœ ìˆê²Œ 300ê°œ
            min_df=2,
            max_df=0.7,
            ngram_range=(1, 2)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)
        except ValueError as e:
            print(f"TF-IDF ë²¡í„°í™” ì˜¤ë¥˜: {e}")
            return set()
        
        # ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜ ê³„ì‚°
        feature_names = vectorizer.get_feature_names_out()
        tfidf_scores = tfidf_matrix.sum(axis=0).A1
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        word_scores = list(zip(feature_names, tfidf_scores))
        word_scores.sort(key=lambda x: x[1], reverse=True)
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ì„  ì„ íƒ
        whitelist_keywords = []
        tfidf_keywords = []
        
        for word, score in word_scores:
            cleaned_word = self._clean_keyword(word)
            
            # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ ìš°ì„ 
            if any(white_term in cleaned_word for white_term in self.music_whitelist):
                whitelist_keywords.append(cleaned_word)
            # ìŒì•… ê¸°í˜¸ë‚˜ ì½”ë“œ íŒ¨í„´
            elif self._is_music_pattern(cleaned_word):
                tfidf_keywords.append(cleaned_word)
            # ì¼ë°˜ TF-IDF í‚¤ì›Œë“œ
            elif len(cleaned_word) >= 3 and score > 0:
                tfidf_keywords.append(cleaned_word)
        
        # ìµœì¢… í‚¤ì›Œë“œ ì„ íƒ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ì„ )
        final_keywords = set()
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ëª¨ë‘ í¬í•¨
        final_keywords.update(whitelist_keywords[:top_n])
        
        # ë‚¨ì€ ê³µê°„ì„ TF-IDF í‚¤ì›Œë“œë¡œ ì±„ì›€
        remaining = top_n - len(final_keywords)
        if remaining > 0:
            final_keywords.update(tfidf_keywords[:remaining])
        
        print(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ: {len(whitelist_keywords)}ê°œ")
        print(f"íŒ¨í„´ ë§¤ì¹­ í‚¤ì›Œë“œ: {len(tfidf_keywords)}ê°œ")
        print(f"ìµœì¢… ì„ íƒ í‚¤ì›Œë“œ: {len(final_keywords)}ê°œ")
        
        return final_keywords
    
    def _is_music_pattern(self, word: str) -> bool:
        """ìŒì•… íŒ¨í„´ ë§¤ì¹­"""
        # ì½”ë“œ íŒ¨í„´ (C, Dm7, G7sus4 ë“±)
        if re.match(r'^[A-G][#â™­]?(maj|min|m|M|dim|aug|sus|add)?\d*$', word):
            return True
        
        # ë¡œë§ˆ ìˆ«ì íŒ¨í„´ (I, ii, V7 ë“±)
        if re.match(r'^[ivIV]+[m]?\d*$', word):
            return True
        
        # ìŒì•… ê¸°í˜¸ í¬í•¨
        if re.search(r'[#â™­â™¯Â°]', word):
            return True
        
        return False
    
    def _clean_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œ ì •ì œ"""
        # ì¡°ì‚¬ ì œê±°
        particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—', 'ì—ì„œ', 'ì˜', 'ë„', 'ë§Œ']
        
        for particle in particles:
            if keyword.endswith(particle) and len(keyword) > len(particle) + 1:
                keyword = keyword[:-len(particle)]
        
        # ê³µë°± ì •ë¦¬
        keyword = ' '.join(keyword.split())
        
        return keyword.strip()
    
    def extract_named_entities(self) -> Set[str]:
        """JSONì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ëœ ê°œë…ë“¤ ì¶”ì¶œ"""
        entities = set()
        
        def extract_entities_recursive(obj, key_path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    # ë©”íƒ€ë°ì´í„°, ìƒìˆ˜ ì œì™¸
                    if key not in ['metadata', 'constants']:
                        # í‚¤ ì´ë¦„ì„ ì—”í‹°í‹°ë¡œ ì¶”ê°€
                        clean_key = key.replace('_', ' ').strip()
                        if len(clean_key) > 2:
                            entities.add(clean_key)
                    
                    # ì¬ê·€ì  íƒìƒ‰
                    if isinstance(value, (dict, list)):
                        extract_entities_recursive(value, f"{key_path}.{key}")
                        
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    extract_entities_recursive(item, f"{key_path}[{i}]")
        
        extract_entities_recursive(self.data)
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì™€ ë§¤ì¹­ë˜ëŠ” ì—”í‹°í‹°ë§Œ í•„í„°ë§
        filtered_entities = set()
        for entity in entities:
            if any(white_term in entity for white_term in self.music_whitelist):
                filtered_entities.add(entity)
        
        print(f"ëª…ì‹œì  ê°œë… ì¶”ì¶œ: {len(filtered_entities)}ê°œ")
        return filtered_entities
    
    def process(self, top_n: int = 100) -> Set[str]:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ ì „ì²´ í”„ë¡œì„¸ìŠ¤
        
        :param top_n: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        :return: ìµœì¢… ì¶”ì¶œëœ í‚¤ì›Œë“œ
        """
        # 1. í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ ì¶”ì¶œ
        corpus = self.extract_text_corpus()
        
        if not corpus:
            print("ì¶”ì¶œí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return set()
        
        # 2. TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        tfidf_keywords = self.extract_keywords_with_tfidf(corpus, top_n)
        
        # 3. ëª…ì‹œì  ê°œë… ì¶”ì¶œ
        named_entities = self.extract_named_entities()
        
        # 4. ë‘ ê²°ê³¼ í•©ì¹˜ê¸°
        all_keywords = tfidf_keywords.union(named_entities)
        
        # 5. í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë³´ì¥
        for white_term in self.music_whitelist:
            # ì½”í¼ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš©ì–´ëŠ” ë¬´ì¡°ê±´ í¬í•¨
            if any(white_term in doc.lower() for doc in corpus):
                all_keywords.add(white_term)
        
        # 6. ìµœì¢… í•„í„°ë§
        final_keywords = self._final_filter(all_keywords, top_n)
        
        print(f"\nìµœì¢… í‚¤ì›Œë“œ: {len(final_keywords)}ê°œ")
        
        return final_keywords
    
    def _final_filter(self, keywords: Set[str], top_n: int) -> Set[str]:
        """ìµœì¢… í•„í„°ë§ ë° ìš°ì„ ìˆœìœ„ ì •ë ¬"""
        scored_keywords = []
        
        for keyword in keywords:
            score = 0
            
            # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì ìˆ˜
            if keyword in self.music_whitelist:
                score += 10
            elif any(white_term in keyword for white_term in self.music_whitelist):
                score += 5
            
            # ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²ƒ í˜ë„í‹°)
            if 3 <= len(keyword) <= 15:
                score += 2
            
            # ìŒì•… íŒ¨í„´ ì ìˆ˜
            if self._is_music_pattern(keyword):
                score += 3
            
            if score > 0:
                scored_keywords.append((keyword, score))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scored_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ Nê°œ ì„ íƒ
        return set(kw[0] for kw in scored_keywords[:top_n])
    
    def save_keywords(self, keywords: Set[str]):
        """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        keywords_dir = 'data/fine_tuning/keywords'
        os.makedirs(keywords_dir, exist_ok=True)
        
        keywords_file = os.path.join(keywords_dir, 'extracted_keywords.json')
        
        # Setì„ Listë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ê¸¸ì´ìˆœ ì •ë ¬)
        keywords_list = sorted(list(keywords), key=lambda x: (-len(x.split()), x))
        
        with open(keywords_file, 'w', encoding='utf-8') as f:
            json.dump(keywords_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‚¤ì›Œë“œ {len(keywords_list)}ê°œ ì €ì¥ ì™„ë£Œ: {keywords_file}")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ“Œ ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
        for i, keyword in enumerate(keywords_list[:20], 1):
            print(f"{i:2d}. {keyword}")
        
        return keywords_list

def main():
    extractor = KeywordExtractor()
    keywords = extractor.process(top_n=100)
    
    # í‚¤ì›Œë“œ ì €ì¥
    extractor.save_keywords(keywords)

if __name__ == "__main__":
    main()
class KeywordExtractor:
    def __init__(self, json_path='data/raw/music_theory_curriculum.json'):
        """
        JSON íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ì´ˆê¸°í™”
        
        :param json_path: JSON íŒŒì¼ ê²½ë¡œ
        """
        self.json_path = json_path
        self.data = self.load_json_data()
        self.keywords = set()
    
    def load_json_data(self) -> Dict:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as file:
                return json.load(file)
        except FileNotFoundError:
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.json_path}")
            return {}
        except json.JSONDecodeError:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {self.json_path}")
            return {}
    
    def extract_text_corpus(self) -> List[str]:
        """JSON ë°ì´í„°ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        corpus = []
        
        def extract_text_recursive(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if isinstance(value, str) and len(value.strip()) > 5:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        corpus.append(value.strip())
                    elif isinstance(value, (dict, list)):
                        extract_text_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_text_recursive(item)
        
        extract_text_recursive(self.data)
        print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(corpus)}")
        return corpus
    
    def extract_keywords_with_tfidf(self, corpus: List[str], top_n: int = 200) -> Set[str]:
        """
        TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìŒì•… ì´ë¡  íŠ¹í™”)
        
        :param corpus: í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤
        :param top_n: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        :return: ì¶”ì¶œëœ í‚¤ì›Œë“œ ì„¸íŠ¸
        """
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        def preprocess(text):
            # ìŒì•… ê¸°í˜¸ ë³´ì¡´
            text = re.sub(r'[^\w\s#â™­â™¯Â°]', ' ', text)  # ìŒì•… ê¸°í˜¸ëŠ” ë³´ì¡´
            return text.lower()
        
        # í•œêµ­ì–´ + ì˜ì–´ ë¶ˆìš©ì–´
        korean_stopwords = ['ì˜', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì—', 'ì„œ', 'ë¡œ', 'ê³¼', 'ì™€', 'ë“±', 'ë°', 'ë˜í•œ', 'ë•Œë¬¸ì—', 'í†µí•´', 'ìœ„í•´', 'ë•Œ']
        english_stopwords = ['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
        all_stopwords = korean_stopwords + english_stopwords
        
        # ì½”í¼ìŠ¤ ì „ì²˜ë¦¬
        preprocessed_corpus = [preprocess(doc) for doc in corpus]
        
        # TF-IDF ë²¡í„°í™” (ì„¤ì • ëŒ€í­ í™•ì¥)
        vectorizer = TfidfVectorizer(
            stop_words=all_stopwords,
            max_features=500,  # 100 â†’ 500ìœ¼ë¡œ ì¦ê°€
            min_df=2,          # ìµœì†Œ 2ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´
            max_df=0.8,        # 80% ì´ìƒ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì œì™¸
            ngram_range=(1, 2) # 1-gram, 2-gram ëª¨ë‘ ì‚¬ìš©
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)
        except ValueError as e:
            print(f"TF-IDF ë²¡í„°í™” ì˜¤ë¥˜: {e}")
            return set()
        
        # ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜ ê³„ì‚°
        feature_names = vectorizer.get_feature_names_out()
        tfidf_scores = tfidf_matrix.sum(axis=0).A1
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        word_scores = list(zip(feature_names, tfidf_scores))
        word_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìŒì•… ì´ë¡  ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì„ íƒ
        music_keywords = []
        general_keywords = []
        
        for word, score in word_scores:
            # ìŒì•… ì´ë¡  í‚¤ì›Œë“œ íŒë³„ (ë” í¬ê´„ì ìœ¼ë¡œ)
            if self._is_music_theory_keyword(word):
                music_keywords.append(word)
            else:
                general_keywords.append(word)
        
        # ìŒì•… í‚¤ì›Œë“œ ìš°ì„ , ë¶€ì¡±í•˜ë©´ ì¼ë°˜ í‚¤ì›Œë“œë¡œ ì±„ì›€
        selected_keywords = music_keywords[:int(top_n * 0.8)]  # 80%ëŠ” ìŒì•… í‚¤ì›Œë“œ
        if len(selected_keywords) < top_n:
            remaining = top_n - len(selected_keywords)
            selected_keywords.extend(general_keywords[:remaining])
        
        print(f"ìŒì•… ì´ë¡  í‚¤ì›Œë“œ: {len(music_keywords)}ê°œ")
        print(f"ì¼ë°˜ í‚¤ì›Œë“œ: {len(general_keywords)}ê°œ")
        print(f"ì„ íƒëœ í‚¤ì›Œë“œ: {len(selected_keywords)}ê°œ")
        
        return set(selected_keywords)
    
    
    def _is_music_theory_keyword(self, word: str) -> bool:
        """ìŒì•… ì´ë¡  í‚¤ì›Œë“œì¸ì§€ íŒë³„"""
        # JSON ë°ì´í„°ì—ì„œ ìŒì•… íŒ¨í„´ ë™ì  ì¶”ì¶œ
        music_patterns = self._extract_music_patterns()
        
        # ìŒì•… ê¸°í˜¸ë‚˜ ìˆ«ì ì¡°í•©
        if re.search(r'[#â™­â™¯Â°]', word) or re.search(r'\d+', word):
            return True
            
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in music_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return True
                
        return False
    
    def _extract_music_patterns(self) -> List[str]:
        """JSON ë°ì´í„°ì—ì„œ ìŒì•… ê´€ë ¨ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = set()
        
        def extract_patterns_recursive(obj):
            if isinstance(obj, dict):
                for key in ['title', 'name', 'concept', 'term']:
                    if key in obj and isinstance(obj[key], str):
                        # í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
                        words = re.findall(r'\b[\wê°€-í£]+\b', obj[key])
                        for word in words:
                            if len(word) > 2:
                                patterns.add(f'.*{word}.*')
                
                for value in obj.values():
                    extract_patterns_recursive(value)
                    
            elif isinstance(obj, list):
                for item in obj:
                    extract_patterns_recursive(item)
        
        extract_patterns_recursive(self.data)
        return list(patterns)
    
    def extract_named_entities(self) -> Set[str]:
        """JSONì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ëœ ê°œë…ë“¤ ì¶”ì¶œ"""
        entities = set()
        
        def extract_entities_recursive(obj, key_path=""):
            if isinstance(obj, dict):
                # ì œëª©, ì´ë¦„, ì •ì˜ ë“±ì˜ í•„ë“œì—ì„œ ì§ì ‘ ì¶”ì¶œ
                for key in ['title', 'name', 'concept', 'definition', 'term']:
                    if key in obj and isinstance(obj[key], str):
                        entity = obj[key].strip()
                        if len(entity) > 1:
                            entities.add(entity)
                
                # ì¬ê·€ì  íƒìƒ‰
                for k, v in obj.items():
                    extract_entities_recursive(v, f"{key_path}.{k}")
                    
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    extract_entities_recursive(item, f"{key_path}[{i}]")
        
        extract_entities_recursive(self.data)
        print(f"ëª…ì‹œì  ê°œë… ì¶”ì¶œ: {len(entities)}ê°œ")
        return entities
    
    def process(self, top_n: int = 200) -> Set[str]:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ ì „ì²´ í”„ë¡œì„¸ìŠ¤
        
        :param top_n: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
        :return: ìµœì¢… ì¶”ì¶œëœ í‚¤ì›Œë“œ
        """
        # 1. í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ ì¶”ì¶œ
        corpus = self.extract_text_corpus()
        
        if not corpus:
            print("ì¶”ì¶œí•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return set()
        
        # 2. TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        tfidf_keywords = self.extract_keywords_with_tfidf(corpus, top_n)
        
        # 3. ëª…ì‹œì  ê°œë… ì¶”ì¶œ
        named_entities = self.extract_named_entities()
        
        # 4. ë‘ ê²°ê³¼ í•©ì¹˜ê¸°
        all_keywords = tfidf_keywords.union(named_entities)
        
        print(f"TF-IDF í‚¤ì›Œë“œ: {len(tfidf_keywords)}ê°œ")
        print(f"ëª…ì‹œì  ê°œë…: {len(named_entities)}ê°œ")
        print(f"ì´ í‚¤ì›Œë“œ: {len(all_keywords)}ê°œ")
        
        return all_keywords
    
    def save_keywords(self, keywords: Set[str]):
        """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        keywords_dir = 'data/fine_tuning/keywords'
        os.makedirs(keywords_dir, exist_ok=True)
        
        keywords_file = os.path.join(keywords_dir, 'extracted_keywords.json')
        
        # Setì„ Listë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ê¸¸ì´ìˆœ ì •ë ¬)
        keywords_list = sorted(list(keywords), key=len, reverse=True)
        
        with open(keywords_file, 'w', encoding='utf-8') as f:
            json.dump(keywords_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‚¤ì›Œë“œ {len(keywords_list)}ê°œ ì €ì¥ ì™„ë£Œ: {keywords_file}")
        return keywords_list

def main():
    extractor = KeywordExtractor()
    keywords = extractor.process(top_n=200)  # 200ê°œë¡œ ì¦ê°€
    
    print("\nì¶”ì¶œëœ í‚¤ì›Œë“œ (ìƒìœ„ 20ê°œ):")
    sorted_keywords = sorted(keywords, key=len, reverse=True)
    for i, keyword in enumerate(sorted_keywords[:20], 1):
        print(f"{i:2d}. {keyword}")
    
    # í‚¤ì›Œë“œ ì €ì¥
    extractor.save_keywords(keywords)

if __name__ == "__main__":
    main()