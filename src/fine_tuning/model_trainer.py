import os
import sys
import json
from typing import List, Dict
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.main import initialize_system
from src.fine_tuning.utils.evaluator import FineTuningEvaluator
from src.fine_tuning.utils.model_updater import ModelUpdater

class ModelTrainer:  # í´ë˜ìŠ¤ëª… ë³€ê²½
    def __init__(self):
        """ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶° ê²½ë¡œ ë³€ê²½
        self.fine_tuning_base = 'data/fine_tuning'
        self.questions_dir = os.path.join(self.fine_tuning_base, 'questions')
        self.evaluations_dir = os.path.join(self.fine_tuning_base, 'evaluations')
        self.reports_dir = os.path.join(self.fine_tuning_base, 'reports')
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.evaluations_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.rag_model = None
        self.evaluator = FineTuningEvaluator()
        self.model_updater = ModelUpdater()
        
        # ì„¸ì…˜ ë°ì´í„°
        self.session_data = {
            'start_time': datetime.now().isoformat(),
            'questions_used': [],
            'evaluations': [],
            'improvements_made': []
        }
    
    def run_training(self):  # ë©”ì„œë“œëª… ë³€ê²½
        """ëª¨ë¸ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸµ ìŒì•… ì´ë¡  ëª¨ë¸ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print("="*60)
        
        # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\n1ï¸âƒ£ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self._initialize_rag_system()
        
        # 2. ì§ˆë¬¸ ë¡œë“œ
        print("\n2ï¸âƒ£ ìƒì„±ëœ ì§ˆë¬¸ ë¡œë“œ...")
        questions = self._load_questions()
        
        if not questions:
            print("âŒ ì‚¬ìš©í•  ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ì§ˆë¬¸ ìƒì„±ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            return
        
        # 3. ë‹µë³€ ìƒì„± ë° í‰ê°€
        print("\n3ï¸âƒ£ ë‹µë³€ ìƒì„± ë° í‰ê°€...")
        self._evaluate_answers(questions)
        
        # 4. ì„¸ì…˜ ì €ì¥
        print("\n4ï¸âƒ£ í‰ê°€ ê²°ê³¼ ì €ì¥...")
        self.evaluator.save_session()
        self._save_session_data()
        
        # 5. ëª¨ë¸ ì—…ë°ì´íŠ¸
        print("\n5ï¸âƒ£ ëª¨ë¸ ì—…ë°ì´íŠ¸...")
        self._update_model_if_needed()
        
        print("\nâœ… ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        self._print_summary()
    
    def _initialize_rag_system(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # initialize_system ì‚¬ìš© (main.pyì—ì„œ)
            self.rag_model = initialize_system()
            
            if self.rag_model is None:
                raise Exception("RAG ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _load_questions(self) -> List[str]:
        """ìƒì„±ëœ ì§ˆë¬¸ë“¤ ë¡œë“œ"""
        # ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œ ì§ˆë¬¸ íŒŒì¼ ìœ„ì¹˜
        possible_files = [
            os.path.join(self.questions_dir, 'refined_questions.json'),
            os.path.join(self.questions_dir, 'raw_questions.json'),
        ]
        
        for questions_file in possible_files:
            if os.path.exists(questions_file):
                try:
                    with open(questions_file, 'r', encoding='utf-8') as f:
                        questions = json.load(f)
                    
                    print(f"âœ… {len(questions)}ê°œì˜ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ: {questions_file}")
                    self.session_data['questions_used'] = questions
                    return questions
                    
                except Exception as e:
                    print(f"âŒ ì§ˆë¬¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ({questions_file}): {e}")
                    continue
        
        print(f"âŒ ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™•ì¸ëœ ê²½ë¡œ:")
        for file_path in possible_files:
            print(f"   - {file_path}: {'ì¡´ì¬' if os.path.exists(file_path) else 'ì—†ìŒ'}")
        
        return []
    
    def _evaluate_answers(self, questions: List[str]):
        """ë‹µë³€ ìƒì„± ë° í‰ê°€"""
        print(f"\nì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í‰ê°€í•©ë‹ˆë‹¤.")
        print("âš ï¸  ë°ì´í„°ì…‹ ê¸°ë°˜ ë‹µë³€ë§Œ ìƒì„±ë˜ë©°, ë°ì´í„° ë¶€ì¡± ì‹œ ëª…í™•íˆ í‘œì‹œë©ë‹ˆë‹¤.")
        
        # í‰ê°€í•  ì§ˆë¬¸ ë²”ìœ„ ì„ íƒ
        try:
            start_idx = int(input(f"ì‹œì‘ ë²ˆí˜¸ (1-{len(questions)}, ê¸°ë³¸ 1): ") or 1) - 1
            end_idx = int(input(f"ë ë²ˆí˜¸ (1-{len(questions)}, ê¸°ë³¸ {min(10, len(questions))}): ") or min(10, len(questions)))
        except ValueError:
            start_idx = 0
            end_idx = min(10, len(questions))
        
        # ë²”ìœ„ ê²€ì¦
        start_idx = max(0, start_idx)
        end_idx = min(len(questions), end_idx)
        
        selected_questions = questions[start_idx:end_idx]
        
        for idx, question in enumerate(selected_questions, start_idx + 1):
            print(f"\n{'='*80}")
            print(f"ì§ˆë¬¸ {idx}/{len(questions)}: {question}")
            print('='*80)
            
            # RAG ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            print("\nğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
            try:
                response = self.rag_model.get_conversation_response(question)
                
                # ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if isinstance(response, dict):
                    answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                    sources = response.get('sources', [])
                    confidence = response.get('confidence', 'unknown')
                    coverage = response.get('data_coverage', 'unknown')
                    
                    # ë‹µë³€ ì¶œë ¥
                    print(f"\nğŸ’¡ ëª¨ë¸ ì‘ë‹µ:")
                    print(answer)
                    
                    # ì°¸ê³ ìë£Œ ì¶œë ¥
                    if sources:
                        print(f"\nğŸ“š ì°¸ê³ ìë£Œ:")
                        for i, source in enumerate(sources, 1):
                            title = source.get('title', 'ì œëª© ì—†ìŒ')
                            content = source.get('content', 'ë‚´ìš© ì—†ìŒ')
                            score = source.get('score', 0)
                            
                            # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
                            if len(content) > 200:
                                content_preview = content[:200] + "..."
                            else:
                                content_preview = content
                            
                            print(f"\n  [{i}] {title} (ìœ ì‚¬ë„: {score:.3f})")
                            print(f"      ë‚´ìš©: {content_preview}")
                    else:
                        print("\nğŸ“š ì°¸ê³ ìë£Œ: ì—†ìŒ")
                    
                    # ë©”íƒ€ë°ì´í„° ì¶œë ¥
                    print(f"\nğŸ“Š ë©”íƒ€ì •ë³´:")
                    print(f"  - ì‹ ë¢°ë„: {confidence}")
                    print(f"  - ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {coverage}")
                    
                    # ë°ì´í„° ì»¤ë²„ë¦¬ì§€ì— ë”°ë¥¸ ì²˜ë¦¬
                    if coverage == 'none':
                        print("\nâ„¹ï¸  ì´ ì§ˆë¬¸ì€ ë°ì´í„°ì…‹ì— ì •ë³´ê°€ ì—†ì–´ ë‹µë³€í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
                        skip_eval = input("í‰ê°€ë¥¼ ê±´ë„ˆë›°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
                        
                        if skip_eval:
                            # ê°­ ë°ì´í„°ë¡œ ê¸°ë¡
                            gap_data = {
                                'question': question,
                                'skipped': True,
                                'reason': 'no_data',
                                'timestamp': datetime.now().isoformat()
                            }
                            self.session_data['evaluations'].append(gap_data)
                            continue
                    
                    # ë‹µë³€ í‰ê°€
                    evaluation = self.evaluator.evaluate_answer(question, answer, sources)
                    
                    # í‰ê°€ ì €ì¥
                    self.evaluator.save_evaluation(evaluation)
                    self.session_data['evaluations'].append(evaluation)
                    
                    print(f"\nâœ… í‰ê°€ ì™„ë£Œ: {evaluation['avg_score']:.1f}/10")
                    
                else:
                    print(f"âŒ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {type(response)}")
                    print(f"ì‘ë‹µ ë‚´ìš©: {response}")
                    continue
                
                # ê³„ì† ì§„í–‰ ì—¬ë¶€
                if idx < start_idx + len(selected_questions):
                    cont = input("\në‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
                    if cont != 'y':
                        break
                        
            except Exception as e:
                print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ì§„í–‰í• ì§€ ì„ íƒ
                cont = input("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
                if cont != 'y':
                    break
                continue
    
    def _save_session_data(self):
        """ì„¸ì…˜ ë°ì´í„° ì €ì¥"""
        self.session_data['end_time'] = datetime.now().isoformat()
        
        # í‰ê°€ í†µê³„ ê³„ì‚°
        evaluations = [e for e in self.session_data['evaluations'] if not e.get('skipped', False)]
        
        if evaluations:
            avg_score = sum(e.get('avg_score', 0) for e in evaluations) / len(evaluations)
            low_quality_count = len([e for e in evaluations if e.get('avg_score', 0) < 7])
            
            self.session_data['statistics'] = {
                'total_evaluations': len(evaluations),
                'average_score': avg_score,
                'low_quality_count': low_quality_count,
                'skipped_count': len(self.session_data['evaluations']) - len(evaluations),
                'improvement_needed': low_quality_count > 0
            }
        
        # ì„¸ì…˜ íŒŒì¼ì„ reports í´ë”ì— ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_file = os.path.join(self.reports_dir, f'training_session_{timestamp}.json')
        
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(self.session_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì„¸ì…˜ ë°ì´í„° ì €ì¥: {session_file}")
    
    def _update_model_if_needed(self):
        """í•„ìš”ì‹œ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        # ê°œì„ ì´ í•„ìš”í•œ í‰ê°€ í™•ì¸
        evaluations = [e for e in self.session_data['evaluations'] if not e.get('skipped', False)]
        poor_evaluations = [
            e for e in evaluations 
            if e.get('avg_score', 0) < 7 and e.get('correction')
        ]
        
        if not poor_evaluations:
            print("âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nâš ï¸ {len(poor_evaluations)}ê°œì˜ ë‹µë³€ì´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì‚¬ìš©ì í™•ì¸
        update_choice = input("ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        
        if update_choice.lower() == 'y':
            try:
                # ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤í–‰
                print("\nğŸ”„ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¤‘...")
                
                # poor_evaluationsë¥¼ correction í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
                corrections = []
                for eval_data in poor_evaluations:
                    correction = {
                        'question': eval_data.get('question', ''),
                        'original_response': eval_data.get('answer', ''),
                        'corrected_response': eval_data.get('correction', ''),
                        'avg_score': eval_data.get('avg_score', 0),
                        'scores': eval_data.get('scores', {}),
                        'feedback': eval_data.get('feedback', ''),
                        'timestamp': eval_data.get('timestamp', datetime.now().isoformat())
                    }
                    corrections.append(correction)
                
                # ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ìˆ˜ì •ëœ ë°©ì‹)
                for correction in corrections:
                    self.model_updater.update_model_data(correction)
                
                # ì—…ë°ì´íŠ¸ ê²°ê³¼ ê¸°ë¡
                self.session_data['improvements_made'] = self.model_updater.update_history
                
                print("âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("ëª¨ë¸ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    def _print_summary(self):
        """íŒŒì¸íŠœë‹ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*60)
        print("ğŸ“Š ëª¨ë¸ íŒŒì¸íŠœë‹ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        stats = self.session_data.get('statistics', {})
        
        if stats:
            print(f"ì´ í‰ê°€ ì§ˆë¬¸: {stats.get('total_evaluations', 0)}ê°œ")
            print(f"í‰ê·  ì ìˆ˜: {stats.get('average_score', 0):.2f}/10")
            print(f"ê°œì„  í•„ìš”: {stats.get('low_quality_count', 0)}ê°œ")
            print(f"ê±´ë„ˆë›´ ì§ˆë¬¸: {stats.get('skipped_count', 0)}ê°œ")
            
            if self.session_data.get('improvements_made'):
                print(f"ëª¨ë¸ ì—…ë°ì´íŠ¸: {len(self.session_data['improvements_made'])}ê°œ ë³€ê²½ì‚¬í•­ ì ìš©")
        
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("- ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë¼ë©´ ì›¹ ì•± ì‹¤í–‰: python app.py")
        print("- ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•˜ë‹¤ë©´ ëª¨ë¸ íŒŒì¸íŠœë‹ ì¬ì‹¤í–‰")
        print("- ë°ì´í„° ê°­ì´ ë§ë‹¤ë©´ ì›ë³¸ ë°ì´í„° í™•ì¥ ê³ ë ¤")

def main():
    """ëª¨ë¸ íŒŒì¸íŠœë‹ ë©”ì¸ ì‹¤í–‰"""
    try:
        trainer = ModelTrainer()  # ë³€ê²½ëœ í´ë˜ìŠ¤ëª…
        trainer.run_training()   # ë³€ê²½ëœ ë©”ì„œë“œëª…
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ëª¨ë¸ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()