import json
import os
from datetime import datetime
from typing import Dict, List
import pandas as pd

class Evaluator:
    def __init__(self, base_path='data/fine_tuning'):
        """
        í‰ê°€ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        :param base_path: íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        """
        self.base_path = base_path
        self.evaluations_path = os.path.join(base_path, 'evaluations')
        self.corrections_path = os.path.join(base_path, 'corrections')
        self.keywords_path = os.path.join(base_path, 'keywords')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # í‰ê°€ ë°ì´í„° ìºì‹œ
        self.current_session_evaluations = []
    
    def _create_directories(self):
        """
        í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        """
        for path in [self.evaluations_path, self.corrections_path, self.keywords_path]:
            os.makedirs(path, exist_ok=True)
    
    def save_evaluation(self, evaluation_data: Dict):
        """
        í‰ê°€ ë°ì´í„° ì €ì¥
        
        :param evaluation_data: í‰ê°€ ë°ì´í„°
        """
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        evaluation_data['timestamp'] = datetime.now().isoformat()
        
        # ì„¸ì…˜ ìºì‹œì— ì¶”ê°€
        self.current_session_evaluations.append(evaluation_data)
        
        # ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"eval_{timestamp}_{len(self.current_session_evaluations)}.json"
        filepath = os.path.join(self.evaluations_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(evaluation_data, f, ensure_ascii=False, indent=2)
        
        # ë‚®ì€ ì ìˆ˜ í‰ê°€ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬
        if evaluation_data.get('avg_score', 0) < 7:
            self._handle_low_score_evaluation(evaluation_data)
        
        print(f"âœ… í‰ê°€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
    
    def _handle_low_score_evaluation(self, evaluation_data: Dict):
        """
        ë‚®ì€ ì ìˆ˜ í‰ê°€ ì²˜ë¦¬
        
        :param evaluation_data: í‰ê°€ ë°ì´í„°
        """
        # ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš° corrections í´ë”ì— ì €ì¥
        if evaluation_data.get('correction'):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"correction_{timestamp}.json"
            filepath = os.path.join(self.corrections_path, filename)
            
            correction_data = {
                'question': evaluation_data['question'],
                'original_response': evaluation_data['response'],
                'corrected_response': evaluation_data['correction'],
                'scores': evaluation_data['scores'],
                'feedback': evaluation_data['feedback'],
                'timestamp': evaluation_data['timestamp']
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(correction_data, f, ensure_ascii=False, indent=2)
    
    def save_session_summary(self):
        """
        í˜„ì¬ ì„¸ì…˜ì˜ ì „ì²´ í‰ê°€ ìš”ì•½ ì €ì¥
        """
        if not self.current_session_evaluations:
            print("ì €ì¥í•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¸ì…˜ ìš”ì•½ ìƒì„±
        session_summary = {
            'session_date': datetime.now().isoformat(),
            'total_evaluations': len(self.current_session_evaluations),
            'avg_score': sum(e['avg_score'] for e in self.current_session_evaluations) / len(self.current_session_evaluations),
            'criteria_averages': self._calculate_criteria_averages(),
            'low_score_questions': self._get_low_score_questions(),
            'evaluations': self.current_session_evaluations
        }
        
        # ìš”ì•½ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"session_summary_{timestamp}.json"
        filepath = os.path.join(self.evaluations_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(session_summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ì„¸ì…˜ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def _calculate_criteria_averages(self) -> Dict[str, float]:
        """
        ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        
        :return: ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜
        """
        criteria = ['accuracy', 'completeness', 'clarity', 'expertise', 'practicality']
        averages = {}
        
        for criterion in criteria:
            scores = [e['scores'][criterion] for e in self.current_session_evaluations]
            averages[criterion] = sum(scores) / len(scores) if scores else 0
        
        return averages
    
    def _get_low_score_questions(self) -> List[Dict]:
        """
        ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ì§ˆë¬¸ë“¤ ì¶”ì¶œ
        
        :return: ë‚®ì€ ì ìˆ˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        return [
            {
                'question': e['question'],
                'avg_score': e['avg_score'],
                'feedback': e['feedback']
            }
            for e in self.current_session_evaluations
            if e['avg_score'] < 7
        ]
    
    def load_evaluation_history(self, limit: int = 10) -> List[Dict]:
        """
        ê³¼ê±° í‰ê°€ ì´ë ¥ ë¡œë“œ
        
        :param limit: ë¡œë“œí•  í‰ê°€ ìˆ˜
        :return: í‰ê°€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        evaluations = []
        
        # ëª¨ë“  í‰ê°€ íŒŒì¼ ë¡œë“œ
        eval_files = sorted(
            [f for f in os.listdir(self.evaluations_path) if f.startswith('eval_')],
            reverse=True
        )[:limit]
        
        for filename in eval_files:
            filepath = os.path.join(self.evaluations_path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                evaluations.append(json.load(f))
        
        return evaluations
    
    def generate_improvement_report(self):
        """
        ëª¨ë¸ ê°œì„ ì„ ìœ„í•œ ë¦¬í¬íŠ¸ ìƒì„±
        """
        # ëª¨ë“  ìˆ˜ì • ë°ì´í„° ë¡œë“œ
        corrections = []
        for filename in os.listdir(self.corrections_path):
            if filename.startswith('correction_'):
                filepath = os.path.join(self.corrections_path, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    corrections.append(json.load(f))
        
        if not corrections:
            print("ìˆ˜ì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'report_date': datetime.now().isoformat(),
            'total_corrections': len(corrections),
            'common_issues': self._analyze_common_issues(corrections),
            'improvement_suggestions': self._generate_suggestions(corrections),
            'corrections': corrections
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"improvement_report_{timestamp}.json"
        filepath = os.path.join(self.base_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ê°œì„  ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {filename}")
    
    def _analyze_common_issues(self, corrections: List[Dict]) -> Dict:
        """
        ê³µí†µ ë¬¸ì œì  ë¶„ì„
        
        :param corrections: ìˆ˜ì • ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        :return: ê³µí†µ ë¬¸ì œì  ë¶„ì„ ê²°ê³¼
        """
        # ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì€ ê¸°ì¤€ë“¤ ë¶„ì„
        low_score_criteria = {}
        
        for correction in corrections:
            scores = correction.get('scores', {})
            for criterion, score in scores.items():
                if score < 7:
                    low_score_criteria[criterion] = low_score_criteria.get(criterion, 0) + 1
        
        return low_score_criteria
    
    def _generate_suggestions(self, corrections: List[Dict]) -> List[str]:
        """
        ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±
        
        :param corrections: ìˆ˜ì • ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        :return: ê°œì„  ì œì•ˆì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        suggestions = []
        
        # í”¼ë“œë°± ë¶„ì„
        feedbacks = [c.get('feedback', '') for c in corrections if c.get('feedback')]
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì œì•ˆ
        if any('ìš©ì–´' in f for f in feedbacks):
            suggestions.append("ìŒì•… ì´ë¡  ìš©ì–´ ì‚¬ìš©ì˜ ì •í™•ì„± ê°œì„  í•„ìš”")
        
        if any('ì˜ˆì‹œ' in f for f in feedbacks):
            suggestions.append("êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì¶”ê°€ í•„ìš”")
        
        if any('ì„¤ëª…' in f for f in feedbacks):
            suggestions.append("ë” ëª…í™•í•˜ê³  ìƒì„¸í•œ ì„¤ëª… í•„ìš”")
        
        return suggestions

def main():
    # í…ŒìŠ¤íŠ¸ìš©
    evaluator = Evaluator()
    
    # ìƒ˜í”Œ í‰ê°€ ë°ì´í„°
    sample_evaluation = {
        'question': 'ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ë€ ë¬´ì—‡ì¸ê°€?',
        'response': 'ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ëŠ”...',
        'sources': [],
        'scores': {
            'accuracy': 8,
            'completeness': 7,
            'clarity': 9,
            'expertise': 8,
            'practicality': 7
        },
        'feedback': 'ë” êµ¬ì²´ì ì¸ ì˜ˆì‹œê°€ í•„ìš”í•©ë‹ˆë‹¤.',
        'correction': '',
        'avg_score': 7.8
    }
    
    # í‰ê°€ ì €ì¥
    evaluator.save_evaluation(sample_evaluation)
    evaluator.save_session_summary()
    evaluator.generate_improvement_report()

if __name__ == "__main__":
    main()