import json
import os
from datetime import datetime
from typing import Dict, List, Optional
import pandas as pd

class FineTuningEvaluator:
    def __init__(self, base_path: str = 'data/fine_tuning'):
        """
        íŒŒì¸íŠœë‹ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        :param base_path: íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        """
        self.base_path = base_path
        self.phase2_path = os.path.join(base_path, 'phase2_model_training')
        self.evaluations_path = os.path.join(self.phase2_path, 'evaluations')
        self.corrections_path = os.path.join(self.phase2_path, 'corrections')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # í‰ê°€ ê¸°ì¤€ ë¡œë“œ
        self.evaluation_criteria = self._load_evaluation_criteria()
        
        # í˜„ì¬ ì„¸ì…˜ ë°ì´í„°
        self.current_session = {
            'start_time': datetime.now().isoformat(),
            'evaluations': [],
            'corrections': []
        }
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for path in [self.evaluations_path, self.corrections_path]:
            os.makedirs(path, exist_ok=True)
    
    def _load_evaluation_criteria(self) -> List[Dict]:
        """ë‹µë³€ í‰ê°€ ê¸°ì¤€ ë¡œë“œ"""
        criteria_path = os.path.join(self.phase2_path, 'answer_criteria.json')
        
        if os.path.exists(criteria_path):
            with open(criteria_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ ë‹µë³€ í‰ê°€ ê¸°ì¤€
            default_criteria = [
                {
                    "key": "accuracy",
                    "name": "ì •í™•ì„±",
                    "description": "ìŒì•… ì´ë¡ ì ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€"
                },
                {
                    "key": "completeness",
                    "name": "ì™„ì „ì„±",
                    "description": "ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„íˆ í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€"
                },
                {
                    "key": "clarity",
                    "name": "ëª…í™•ì„±",
                    "description": "ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€"
                },
                {
                    "key": "relevance",
                    "name": "ê´€ë ¨ì„±",
                    "description": "ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹µë³€ì¸ê°€"
                },
                {
                    "key": "examples",
                    "name": "ì˜ˆì‹œì˜ ì ì ˆì„±",
                    "description": "ì ì ˆí•œ ìŒì•…ì  ì˜ˆì‹œë¥¼ ì œê³µí–ˆëŠ”ê°€"
                }
            ]
            
            # ì €ì¥
            with open(criteria_path, 'w', encoding='utf-8') as f:
                json.dump(default_criteria, f, ensure_ascii=False, indent=2)
            
            return default_criteria
    
    def evaluate_answer(self, question: str, answer: str, sources: List[Dict]) -> Dict:
        """
        ë‹¨ì¼ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ ìˆ˜ì§‘
        
        :param question: ì§ˆë¬¸
        :param answer: ëª¨ë¸ì˜ ë‹µë³€
        :param sources: ì°¸ê³ ìë£Œ
        :return: í‰ê°€ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ ë‹µë³€ í‰ê°€")
        print(f"{'='*60}")
        print(f"ì§ˆë¬¸: {question}")
        print(f"\në‹µë³€:\n{answer}")
        
        # ì°¸ê³ ìë£Œ ì¶œë ¥
        if sources:
            print(f"\nğŸ“š ì°¸ê³ ìë£Œ:")
            for i, source in enumerate(sources, 1):
                print(f"{i}. {source.get('title', 'ì œëª© ì—†ìŒ')} (ìœ ì‚¬ë„: {source.get('score', 0):.3f})")
                content = source.get('content', '')
                print(f"   {content[:100]}...")
        
        print(f"\n{'='*60}")
        
        # í‰ê°€ ì ìˆ˜ ìˆ˜ì§‘
        scores = {}
        for criterion in self.evaluation_criteria:
            print(f"\n{criterion['name']} - {criterion['description']}")
            while True:
                try:
                    score = int(input(f"ì ìˆ˜ (0-10): "))
                    if 0 <= score <= 10:
                        scores[criterion['key']] = score
                        break
                    else:
                        print("0-10 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                except ValueError:
                    print("ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = sum(scores.values()) / len(scores)
        
        print(f"\ní‰ê·  ì ìˆ˜: {avg_score:.1f}/10")
        
        # ìƒì„¸ í”¼ë“œë°±
        feedback = input("\nìƒì„¸ í”¼ë“œë°± (ë¬¸ì œì , ê°œì„ ì‚¬í•­): ")
        
        # ìˆ˜ì • ì œì•ˆ
        correction = ""
        if avg_score < 7:
            print("\nâš ï¸ ê°œì„ ì´ í•„ìš”í•œ ë‹µë³€ì…ë‹ˆë‹¤.")
            correction = input("ìˆ˜ì •ëœ ë‹µë³€ ì œì•ˆ (ì„ íƒì‚¬í•­): ")
        
        # í‰ê°€ ë°ì´í„° êµ¬ì„±
        evaluation = {
            'timestamp': datetime.now().isoformat(),
            'question': question,
            'answer': answer,
            'sources': sources,
            'scores': scores,
            'avg_score': avg_score,
            'feedback': feedback,
            'correction': correction,
            'needs_improvement': avg_score < 7
        }
        
        return evaluation
    
    def save_evaluation(self, evaluation: Dict):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        # ì„¸ì…˜ ë°ì´í„°ì— ì¶”ê°€
        self.current_session['evaluations'].append(evaluation)
        
        # ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_{timestamp}.json"
        filepath = os.path.join(self.evaluations_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(evaluation, f, ensure_ascii=False, indent=2)
        
        # ê°œì„ ì´ í•„ìš”í•œ ê²½ìš° ìˆ˜ì • ë°ì´í„° ì €ì¥
        if evaluation.get('needs_improvement') and evaluation.get('correction'):
            self._save_correction(evaluation)
        
        print(f"âœ… í‰ê°€ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def _save_correction(self, evaluation: Dict):
        """ìˆ˜ì • ë°ì´í„° ì €ì¥"""
        correction_data = {
            'timestamp': evaluation['timestamp'],
            'question': evaluation['question'],
            'original_answer': evaluation['answer'],
            'corrected_answer': evaluation['correction'],
            'scores': evaluation['scores'],
            'feedback': evaluation['feedback']
        }
        
        # ì„¸ì…˜ ë°ì´í„°ì— ì¶”ê°€
        self.current_session['corrections'].append(correction_data)
        
        # íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"correction_{timestamp}.json"
        filepath = os.path.join(self.corrections_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(correction_data, f, ensure_ascii=False, indent=2)
    
    def save_session(self):
        """í˜„ì¬ ì„¸ì…˜ ì €ì¥"""
        if not self.current_session['evaluations']:
            print("ì €ì¥í•  í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¸ì…˜ ìš”ì•½ ê³„ì‚°
        evaluations = self.current_session['evaluations']
        session_summary = {
            'session_info': {
                'start_time': self.current_session['start_time'],
                'end_time': datetime.now().isoformat(),
                'total_evaluations': len(evaluations),
                'total_corrections': len(self.current_session['corrections'])
            },
            'statistics': self._calculate_session_stats(evaluations),
            'evaluations': evaluations,
            'corrections': self.current_session['corrections']
        }
        
        # ì„¸ì…˜ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"session_{timestamp}.json"
        filepath = os.path.join(self.phase2_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(session_summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {filename}")
        self._print_session_summary(session_summary['statistics'])
    
    def _calculate_session_stats(self, evaluations: List[Dict]) -> Dict:
        """ì„¸ì…˜ í†µê³„ ê³„ì‚°"""
        if not evaluations:
            return {}
        
        # ê¸°ë³¸ í†µê³„
        total_count = len(evaluations)
        avg_overall = sum(e['avg_score'] for e in evaluations) / total_count
        
        # ê¸°ì¤€ë³„ í‰ê· 
        criteria_averages = {}
        for criterion in self.evaluation_criteria:
            key = criterion['key']
            scores = [e['scores'][key] for e in evaluations if key in e['scores']]
            if scores:
                criteria_averages[criterion['name']] = sum(scores) / len(scores)
        
        # í’ˆì§ˆ ë¶„í¬
        excellent = len([e for e in evaluations if e['avg_score'] >= 8])
        good = len([e for e in evaluations if 6 <= e['avg_score'] < 8])
        poor = len([e for e in evaluations if e['avg_score'] < 6])
        
        return {
            'total_evaluations': total_count,
            'avg_overall_score': avg_overall,
            'criteria_averages': criteria_averages,
            'quality_distribution': {
                'excellent': {'count': excellent, 'percentage': excellent/total_count*100},
                'good': {'count': good, 'percentage': good/total_count*100},
                'poor': {'count': poor, 'percentage': poor/total_count*100}
            }
        }
    
    def _print_session_summary(self, stats: Dict):
        """ì„¸ì…˜ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š Phase 2 í‰ê°€ ì„¸ì…˜ ìš”ì•½")
        print("="*60)
        
        print(f"ì´ í‰ê°€ ìˆ˜: {stats['total_evaluations']}")
        print(f"ì „ì²´ í‰ê· : {stats['avg_overall_score']:.2f}/10")
        
        print("\nê¸°ì¤€ë³„ í‰ê· :")
        for criterion, avg in stats['criteria_averages'].items():
            print(f"  - {criterion}: {avg:.2f}/10")
        
        print("\ní’ˆì§ˆ ë¶„í¬:")
        dist = stats['quality_distribution']
        print(f"  - ìš°ìˆ˜ (8ì  ì´ìƒ): {dist['excellent']['count']}ê°œ ({dist['excellent']['percentage']:.1f}%)")
        print(f"  - ì–‘í˜¸ (6-8ì ): {dist['good']['count']}ê°œ ({dist['good']['percentage']:.1f}%)")
        print(f"  - ê°œì„  í•„ìš” (6ì  ë¯¸ë§Œ): {dist['poor']['count']}ê°œ ({dist['poor']['percentage']:.1f}%)")
    
    def load_corrections(self) -> List[Dict]:
        """ì €ì¥ëœ ìˆ˜ì • ë°ì´í„° ë¡œë“œ"""
        corrections = []
        
        if not os.path.exists(self.corrections_path):
            return corrections
        
        for filename in os.listdir(self.corrections_path):
            if filename.startswith('correction_') and filename.endswith('.json'):
                filepath = os.path.join(self.corrections_path, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        corrections.append(json.load(f))
                except Exception as e:
                    print(f"ìˆ˜ì • ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ({filename}): {e}")
        
        return sorted(corrections, key=lambda x: x.get('timestamp', ''))
    
    def generate_improvement_report(self) -> Dict:
        """ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±"""
        corrections = self.load_corrections()
        
        if not corrections:
            return {"message": "ê°œì„  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ê³µí†µ ë¬¸ì œì  ë¶„ì„
        common_issues = self._analyze_common_issues(corrections)
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self._generate_improvement_suggestions(corrections)
        
        report = {
            'report_date': datetime.now().isoformat(),
            'total_corrections': len(corrections),
            'common_issues': common_issues,
            'improvement_suggestions': suggestions,
            'correction_details': corrections
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(self.phase2_path, 'improvement_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")
        return report
    
    def _analyze_common_issues(self, corrections: List[Dict]) -> Dict:
        """ê³µí†µ ë¬¸ì œì  ë¶„ì„"""
        # ë‚®ì€ ì ìˆ˜ ê¸°ì¤€ ë¶„ì„
        low_criteria = {}
        
        for correction in corrections:
            scores = correction.get('scores', {})
            for criterion, score in scores.items():
                if score < 7:
                    low_criteria[criterion] = low_criteria.get(criterion, 0) + 1
        
        # í”¼ë“œë°± í‚¤ì›Œë“œ ë¶„ì„
        feedback_keywords = {}
        for correction in corrections:
            feedback = correction.get('feedback', '').lower()
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = ['ë¶€ì •í™•', 'ë¶ˆì™„ì „', 'ë¶ˆëª…í™•', 'ì˜ˆì‹œ', 'ì„¤ëª…']
            for keyword in keywords:
                if keyword in feedback:
                    feedback_keywords[keyword] = feedback_keywords.get(keyword, 0) + 1
        
        return {
            'low_score_criteria': low_criteria,
            'feedback_keywords': feedback_keywords
        }
    
    def _generate_improvement_suggestions(self, corrections: List[Dict]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì œì•ˆ
        common_issues = self._analyze_common_issues(corrections)
        
        low_criteria = common_issues['low_criteria']
        if 'accuracy' in low_criteria:
            suggestions.append("ìŒì•… ì´ë¡  ì •í™•ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if 'examples' in low_criteria:
            suggestions.append("êµ¬ì²´ì ì¸ ìŒì•… ì˜ˆì‹œ ì¶”ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if 'clarity' in low_criteria:
            suggestions.append("ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return suggestions

def main():
    """í‰ê°€ê¸° í…ŒìŠ¤íŠ¸"""
    evaluator = FineTuningEvaluator()
    
    # ìƒ˜í”Œ í‰ê°€
    sample_question = "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ë€ ë¬´ì—‡ì¸ê°€?"
    sample_answer = "ì„¸ì»¨ë”ë¦¬ ë„ë¯¸ë„ŒíŠ¸ëŠ” ì¡°ì„± ë‚´ì—ì„œ ë‹¤ë¥¸ í™”ìŒìœ¼ë¡œì˜ ì¼ì‹œì  ì „ì¡°ë¥¼ ë§Œë“œëŠ” ë„ë¯¸ë„ŒíŠ¸ í™”ìŒì…ë‹ˆë‹¤."
    sample_sources = []
    
    evaluation = evaluator.evaluate_answer(sample_question, sample_answer, sample_sources)
    evaluator.save_evaluation