"""
íŒŒì¸íŠœë‹ í‰ê°€ ì‹œìŠ¤í…œ
- ë‹µë³€ í‰ê°€ ë° ì €ì¥
- ì„¸ì…˜ ê´€ë¦¬
- ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±
"""
import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional

class FineTuningEvaluator:
    def __init__(self, base_path='data/fine_tuning'):
        """
        í‰ê°€ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        :param base_path: íŒŒì¸íŠœë‹ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        """
        self.base_path = base_path
        self.evaluations_path = os.path.join(base_path, 'evaluations')
        self.corrections_path = os.path.join(base_path, 'corrections')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # í‰ê°€ ê¸°ì¤€ ë¡œë“œ
        self.criteria = self._load_evaluation_criteria()
        
        # í‰ê°€ ë°ì´í„° ìºì‹œ
        self.current_session_evaluations = []
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for path in [self.evaluations_path, self.corrections_path]:
            os.makedirs(path, exist_ok=True)
    
    def _load_evaluation_criteria(self) -> List[Dict]:
        """í‰ê°€ ê¸°ì¤€ ë¡œë“œ - ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        # í‰ê°€ ê¸°ì¤€ íŒŒì¼ ê²½ë¡œ
        criteria_path = os.path.join(self.base_path, 'evaluation_criteria.json')
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
        if os.path.exists(criteria_path):
            try:
                with open(criteria_path, 'r', encoding='utf-8') as f:
                    criteria_data = json.load(f)
                    
                # ë°ì´í„° êµ¬ì¡° í™•ì¸ ë° ë³€í™˜
                if isinstance(criteria_data, dict) and 'answer_criteria' in criteria_data:
                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    return criteria_data['answer_criteria']
                elif isinstance(criteria_data, list):
                    # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    return criteria_data
                else:
                    print("âš ï¸ í‰ê°€ ê¸°ì¤€ í˜•ì‹ ì˜¤ë¥˜. ê¸°ë³¸ ê¸°ì¤€ ì‚¬ìš©.")
            except Exception as e:
                print(f"âš ï¸ í‰ê°€ ê¸°ì¤€ ë¡œë“œ ì˜¤ë¥˜: {e}. ê¸°ë³¸ ê¸°ì¤€ ì‚¬ìš©.")
        
        # ê¸°ë³¸ í‰ê°€ ê¸°ì¤€
        return [
            {
                'key': 'source_accuracy',
                'name': 'ì°¸ê³ ìë£Œ ì •í™•ì„±',
                'description': 'ì°¸ê³ ìë£Œë¥¼ ì •í™•íˆ ì¸ìš©í–ˆëŠ”ê°€'
            },
            {
                'key': 'source_citation',
                'name': 'ì¶œì²˜ í‘œì‹œ',
                'description': 'ëª¨ë“  ì •ë³´ì— ì¶œì²˜ê°€ í‘œì‹œë˜ì—ˆëŠ”ê°€'
            },
            {
                'key': 'no_hallucination',
                'name': 'í™˜ê° ì—†ìŒ',
                'description': 'ì°¸ê³ ìë£Œì— ì—†ëŠ” ë‚´ìš©ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ëŠ”ê°€'
            },
            {
                'key': 'clarity',
                'name': 'ëª…í™•ì„±',
                'description': 'ë‹µë³€ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€'
            },
            {
                'key': 'completeness',
                'name': 'ì™„ì „ì„±',
                'description': 'ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í–ˆëŠ”ê°€'
            }
        ]
    
    def evaluate_answer(self, question: str, answer: str, sources: List[Dict]) -> Dict:
        """ì ìˆ˜ë³„ ì „ëµì„ ê³ ë ¤í•œ ë‹µë³€ í‰ê°€"""
        
        print(f"\nğŸ“‹ ë‹µë³€ í‰ê°€: {question}")
        print(f"\nğŸ’¡ í˜„ì¬ ë‹µë³€:\n{answer}")
        
        # ì ìˆ˜ë³„ ì „ëµ ì•ˆë‚´
        print(f"\nğŸ“Š ì ìˆ˜ë³„ ì—…ë°ì´íŠ¸ ì „ëµ:")
        print(f"  ğŸ”´ 0-3ì : ì—…ë°ì´íŠ¸ ì•ˆí•¨")
        print(f"  ğŸŸ¡ 4-5ì : ì™„ì „ êµì²´ (ìƒˆë¡œ ì‘ì„± í•„ìš”)")  
        print(f"  ğŸŸ¢ 6-7ì : ë¯¸ì„¸ ì¡°ì • (ì›ë³¸ + ìˆ˜ì •ì‚¬í•­)")
        print(f"  ğŸ”µ 8-10ì : ì„ íƒì  ê°œì„  (ì›ë³¸ ìœ ì§€ + ì•½ê°„ ë³´ì™„)")
        
        # ê°„í¸ í‰ê°€
        print(f"\nâš¡ ë¹ ë¥¸ í‰ê°€:")
        print(f"1. ì™„ì „íˆ í‹€ë¦¼ (1-3ì ) - ì—…ë°ì´íŠ¸ ì•ˆí•¨")
        print(f"2. ë§ì´ í‹€ë¦¼ (4-5ì ) - ìƒˆë¡œ ì‘ì„±")
        print(f"3. ê´œì°®ì§€ë§Œ ì•„ì‰¬ì›€ (6-7ì ) - ë¯¸ì„¸ ì¡°ì •")  
        print(f"4. ê±°ì˜ ì™„ë²½ (8-10ì ) - ì•½ê°„ë§Œ ë³´ì™„")
        
        choice = input("ì„ íƒ (1-4): ").strip()
        
        if choice == '1':
            # ì—…ë°ì´íŠ¸ ì•ˆí•¨
            scores = {criterion['key']: 2 for criterion in self.criteria}
            feedback = "ë‹µë³€ì´ ë¶€ì ì ˆí•˜ì—¬ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ"
            correction = ""
            
        elif choice == '2':
            # ì™„ì „ êµì²´
            scores = {criterion['key']: 4 for criterion in self.criteria}
            feedback = input("ì–´ë–¤ ì ì´ ë¬¸ì œì¸ê°€ìš”? ")
            print("ğŸ’¡ ì™„ì „íˆ ìƒˆë¡œìš´ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:")
            correction = input("ìƒˆë¡œìš´ ë‹µë³€: ")
            
        elif choice == '3':
            # ë¯¸ì„¸ ì¡°ì • - ê°€ì¥ ë§ì´ ì‚¬ìš©ë  ì¼€ì´ìŠ¤
            scores = {criterion['key']: 6 for criterion in self.criteria}
            feedback = input("ì–´ë–¤ ë¶€ë¶„ì„ ê°œì„ í•˜ë©´ ì¢‹ì„ê¹Œìš”? ")
            print("ğŸ’¡ ì¶”ê°€í•˜ê±°ë‚˜ ìˆ˜ì •í•  ë‚´ìš©ë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”:")
            correction = input("ë¯¸ì„¸ ì¡°ì • ë‚´ìš©: ")
            
        elif choice == '4':
            # ì„ íƒì  ê°œì„ 
            scores = {criterion['key']: 8 for criterion in self.criteria}
            feedback = input("ë” ì¢‹ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì ì´ ìˆë‹¤ë©´? ")
            correction = input("ì•½ê°„ì˜ ë³´ì™„ ë‚´ìš© (ì„ íƒì‚¬í•­): ")
            
        else:
            # ìˆ˜ë™ ì…ë ¥
            print("\nìƒì„¸ í‰ê°€ ëª¨ë“œ:")
            scores = {}
            for criterion in self.criteria:
                while True:
                    try:
                        score = int(input(f"{criterion['name']} (0-10): "))
                        if 0 <= score <= 10:
                            scores[criterion['key']] = score
                            break
                    except ValueError:
                        print("ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
            avg_score = sum(scores.values()) / len(scores)
            print(f"\ní˜„ì¬ í‰ê·  ì ìˆ˜: {avg_score:.1f}")
            
            if avg_score >= 8:
                print("ğŸ’¡ ë†’ì€ ì ìˆ˜ì…ë‹ˆë‹¤. ì•½ê°„ì˜ ë³´ì™„ë§Œ ì…ë ¥í•˜ì„¸ìš”.")
            elif avg_score >= 6:
                print("ğŸ’¡ ì¤‘ê°„ ì ìˆ˜ì…ë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì • ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                print("ğŸ’¡ ë‚®ì€ ì ìˆ˜ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ë‹µë³€ì´ë‚˜ ëŒ€í­ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
            feedback = input("í”¼ë“œë°±: ")
            correction = input("ìˆ˜ì •/ë³´ì™„ ë‚´ìš©: ")
        
        avg_score = sum(scores.values()) / len(scores)
        
        return {
            'question': question,
            'answer': answer,
            'sources': sources,
            'scores': scores,
            'avg_score': avg_score,
            'feedback': feedback,
            'correction': correction,
            'timestamp': datetime.now().isoformat()
        }
        
    def save_evaluation(self, evaluation: Dict):
        """
        í‰ê°€ ë°ì´í„° ì €ì¥ (ëˆ„ì  ë°©ì‹)
        
        :param evaluation: í‰ê°€ ë°ì´í„°
        """
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        evaluation['timestamp'] = datetime.now().isoformat()
        
        # ì„¸ì…˜ ìºì‹œì— ì¶”ê°€
        self.current_session_evaluations.append(evaluation)
        
        # ëˆ„ì  íŒŒì¼ì— ì¶”ê°€
        evaluations_file = os.path.join(self.evaluations_path, "all_evaluations.json")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_evaluations = []
        if os.path.exists(evaluations_file):
            try:
                with open(evaluations_file, 'r', encoding='utf-8') as f:
                    existing_evaluations = json.load(f)
            except json.JSONDecodeError:
                print(f"âš ï¸ ì†ìƒëœ í‰ê°€ íŒŒì¼ ë°œê²¬. ìƒˆ íŒŒì¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ìƒˆ í‰ê°€ ë°ì´í„° ì¶”ê°€
        existing_evaluations.append(evaluation)
        
        # ì „ì²´ ë°ì´í„° ì €ì¥
        with open(evaluations_file, 'w', encoding='utf-8') as f:
            json.dump(existing_evaluations, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‰ê°€ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ (ì´ {len(existing_evaluations)}ê°œ)")
        
        # ë‚®ì€ ì ìˆ˜ í‰ê°€ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬
        if evaluation.get('avg_score', 0) < 7 and evaluation.get('correction'):
            self._handle_low_score_evaluation(evaluation)
    
    def _handle_low_score_evaluation(self, evaluation: Dict):
        """
        ë‚®ì€ ì ìˆ˜ í‰ê°€ ì²˜ë¦¬
        
        :param evaluation: í‰ê°€ ë°ì´í„°
        """
        # ìˆ˜ì •ì´ í•„ìš”í•œ ê²½ìš° corrections í´ë”ì— ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"correction_{timestamp}.json"
        filepath = os.path.join(self.corrections_path, filename)
        
        correction_data = {
            'question': evaluation['question'],
            'original_response': evaluation['answer'],
            'corrected_response': evaluation['correction'],
            'scores': evaluation['scores'],
            'feedback': evaluation['feedback'],
            'timestamp': evaluation['timestamp']
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(correction_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ ìˆ˜ì • ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
    
    def save_session(self):
        """í˜„ì¬ ì„¸ì…˜ ë°ì´í„° ì €ì¥"""
        if not self.current_session_evaluations:
            print("ì €ì¥í•  ì„¸ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„¸ì…˜ ìš”ì•½
        session_summary = {
            'session_date': datetime.now().isoformat(),
            'total_evaluations': len(self.current_session_evaluations),
            'avg_score': sum(e.get('avg_score', 0) for e in self.current_session_evaluations) / len(self.current_session_evaluations),
            'evaluations': self.current_session_evaluations
        }
        
        # ì„¸ì…˜ ìš”ì•½ íŒŒì¼ ê²½ë¡œ
        summaries_file = os.path.join(self.evaluations_path, "session_summaries.json")
        
        # ê¸°ì¡´ ì„¸ì…˜ ìš”ì•½ ë¡œë“œ
        existing_summaries = []
        if os.path.exists(summaries_file):
            try:
                with open(summaries_file, 'r', encoding='utf-8') as f:
                    existing_summaries = json.load(f)
            except json.JSONDecodeError:
                print(f"âš ï¸ ì†ìƒëœ ì„¸ì…˜ ìš”ì•½ íŒŒì¼ ë°œê²¬. ìƒˆ íŒŒì¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ìƒˆ ì„¸ì…˜ ìš”ì•½ ì¶”ê°€
        existing_summaries.append(session_summary)
        
        # ì „ì²´ ë°ì´í„° ì €ì¥
        with open(summaries_file, 'w', encoding='utf-8') as f:
            json.dump(existing_summaries, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ì„¸ì…˜ ìš”ì•½ ì¶”ê°€ ì™„ë£Œ (ì´ {len(existing_summaries)}ê°œ ì„¸ì…˜)")
        
        # ê°œë³„ ì„¸ì…˜ íŒŒì¼ë„ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_file = os.path.join(self.evaluations_path, f"session_{timestamp}.json")
        
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_summary, f, ensure_ascii=False, indent=2)
    
    def get_low_score_evaluations(self, threshold: float = 7.0) -> List[Dict]:
        """
        ë‚®ì€ ì ìˆ˜ì˜ í‰ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        
        :param threshold: ì ìˆ˜ ì„ê³„ê°’
        :return: ë‚®ì€ ì ìˆ˜ì˜ í‰ê°€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        low_score_evals = []
        
        # ì „ì²´ í‰ê°€ ë°ì´í„° ë¡œë“œ
        evaluations_file = os.path.join(self.evaluations_path, "all_evaluations.json")
        if os.path.exists(evaluations_file):
            try:
                with open(evaluations_file, 'r', encoding='utf-8') as f:
                    all_evaluations = json.load(f)
                    
                # ë‚®ì€ ì ìˆ˜ í•„í„°ë§
                low_score_evals = [
                    e for e in all_evaluations 
                    if e.get('avg_score', 0) < threshold
                ]
            except Exception as e:
                print(f"í‰ê°€ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return low_score_evals
    
    def get_all_corrections(self) -> List[Dict]:
        """
        ëª¨ë“  ìˆ˜ì • ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        
        :return: ëª¨ë“  ìˆ˜ì • ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        corrections = []
        
        # corrections ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ë¡œë“œ
        if os.path.exists(self.corrections_path):
            for filename in os.listdir(self.corrections_path):
                if filename.startswith('correction_') and filename.endswith('.json'):
                    filepath = os.path.join(self.corrections_path, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            correction = json.load(f)
                            corrections.append(correction)
                    except Exception as e:
                        print(f"ìˆ˜ì • ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ({filename}): {e}")
        
        return corrections
    
    def generate_improvement_report(self, output_path: Optional[str] = None) -> Dict:
        """
        ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±
        
        :param output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        :return: ë¦¬í¬íŠ¸ ë°ì´í„°
        """
        # ëª¨ë“  í‰ê°€ ë°ì´í„° ë¡œë“œ
        all_evaluations = []
        evaluations_file = os.path.join(self.evaluations_path, "all_evaluations.json")
        if os.path.exists(evaluations_file):
            try:
                with open(evaluations_file, 'r', encoding='utf-8') as f:
                    all_evaluations = json.load(f)
            except Exception as e:
                print(f"í‰ê°€ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ìˆ˜ì • ë°ì´í„° ë¡œë“œ
        all_corrections = self.get_all_corrections()
        
        # í†µê³„ ê³„ì‚°
        total_evals = len(all_evaluations)
        avg_score = sum(e.get('avg_score', 0) for e in all_evaluations) / total_evals if total_evals > 0 else 0
        low_score_count = len([e for e in all_evaluations if e.get('avg_score', 0) < 7])
        
        # ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜
        criteria_scores = {}
        for evaluation in all_evaluations:
            scores = evaluation.get('scores', {})
            for key, score in scores.items():
                if key not in criteria_scores:
                    criteria_scores[key] = []
                criteria_scores[key].append(score)
        
        criteria_avgs = {
            key: sum(scores) / len(scores) if scores else 0 
            for key, scores in criteria_scores.items()
        }
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': {
                'total_evaluations': total_evals,
                'average_score': avg_score,
                'low_score_count': low_score_count,
                'correction_count': len(all_corrections)
            },
            'criteria_averages': criteria_avgs,
            'improvement_areas': [
                {
                    'criterion': key,
                    'average_score': avg,
                    'priority': 'high' if avg < 6 else ('medium' if avg < 7.5 else 'low')
                }
                for key, avg in sorted(criteria_avgs.items(), key=lambda x: x[1])
            ]
        }
        
        # ì €ì¥
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š ê°œì„  ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return report

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    evaluator = FineTuningEvaluator()
    
    # í‰ê°€ ê¸°ì¤€ ì¶œë ¥
    print("ğŸ“‹ í‰ê°€ ê¸°ì¤€:")
    for criterion in evaluator.criteria:
        print(f"- {criterion.get('name')}: {criterion.get('description')}")
    
    # ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report = evaluator.generate_improvement_report(f"data/fine_tuning/improvement_report_{timestamp}.json")
    
    print("\nğŸ“Š ê°œì„  ë¦¬í¬íŠ¸ ìš”ì•½:")
    print(f"ì´ í‰ê°€: {report['statistics']['total_evaluations']}ê°œ")
    print(f"í‰ê·  ì ìˆ˜: {report['statistics']['average_score']:.2f}/10")
    print(f"ê°œì„  í•„ìš”: {report['statistics']['low_score_count']}ê°œ")

if __name__ == "__main__":
    main()