import os
import sys
import json
from typing import List, Dict, Set
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.fine_tuning.utils.question_generator import QuestionGenerator

class Phase1QuestionImprovement:
    def __init__(self):
        """Phase 1: ì§ˆë¬¸ ê°œì„  ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.base_dir = 'data/fine_tuning/questions'
        os.makedirs(self.base_dir, exist_ok=True)
        
        # íŒŒì¼ ê²½ë¡œë“¤ ì—…ë°ì´íŠ¸
        self.raw_questions_file = os.path.join(self.base_dir, 'raw_questions.json')
        self.refined_questions_file = os.path.join(self.base_dir, 'refined_questions.json')
        self.question_criteria_file = os.path.join(self.base_dir, 'question_criteria.json')
        self.evaluations_file = os.path.join(self.base_dir, 'question_evaluations.json')
        
        # í‰ê°€ ê¸°ì¤€ ë¡œë“œ
        self.evaluation_criteria = self._load_evaluation_criteria()
        
        # í˜„ì¬ ì„¸ì…˜ ë°ì´í„°
        self.session_data = {
            'start_time': datetime.now().isoformat(),
            'evaluations': [],
            'improved_questions': []
        }
    
    def _load_evaluation_criteria(self) -> List[Dict]:
        """ì§ˆë¬¸ í‰ê°€ ê¸°ì¤€ ë¡œë“œ"""
        criteria_path = os.path.join(self.base_dir, 'question_criteria.json')
        
        if os.path.exists(criteria_path):
            with open(criteria_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ í‰ê°€ ê¸°ì¤€
            default_criteria = [
                {
                    "key": "clarity",
                    "name": "ëª…í™•ì„±",
                    "description": "ì§ˆë¬¸ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€"
                },
                {
                    "key": "relevance",
                    "name": "ê´€ë ¨ì„±",
                    "description": "í‚¤ì›Œë“œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì´ ë†’ì€ê°€"
                },
                {
                    "key": "difficulty",
                    "name": "ë‚œì´ë„",
                    "description": "ì ì ˆí•œ ë‚œì´ë„ì˜ ì§ˆë¬¸ì¸ê°€"
                },
                {
                    "key": "specificity",
                    "name": "êµ¬ì²´ì„±",
                    "description": "êµ¬ì²´ì ì´ê³  ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ì¸ê°€"
                },
                {
                    "key": "educational",
                    "name": "êµìœ¡ì  ê°€ì¹˜",
                    "description": "í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ì§ˆë¬¸ì¸ê°€"
                }
            ]
            
            # ì €ì¥
            with open(criteria_path, 'w', encoding='utf-8') as f:
                json.dump(default_criteria, f, ensure_ascii=False, indent=2)
            
            return default_criteria
    
    def run_phase1(self):
        """Phase 1 ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸµ Phase 1: ì§ˆë¬¸ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print("="*60)
        
        # 1. ìŠ¹ì¸ëœ í‚¤ì›Œë“œ ë¡œë“œ
        print("\n1ï¸âƒ£ ìŠ¹ì¸ëœ í‚¤ì›Œë“œ ë¡œë“œ ì¤‘...")
        keywords = self._load_approved_keywords()
        
        if not keywords:
            print("âŒ ìŠ¹ì¸ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ì§ˆë¬¸ ìƒì„±
        print("\n2ï¸âƒ£ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        raw_questions = self._generate_questions(keywords)
        
        # 3. ì§ˆë¬¸ í‰ê°€ ë° ê°œì„ 
        print("\n3ï¸âƒ£ ì§ˆë¬¸ í‰ê°€ ë° ê°œì„ ...")
        improved_questions = self._evaluate_and_improve_questions(raw_questions)
        
        # 4. ê²°ê³¼ ì €ì¥
        print("\n4ï¸âƒ£ ê²°ê³¼ ì €ì¥ ì¤‘...")
        self._save_results(improved_questions)
        
        print("\nâœ… Phase 1 ì™„ë£Œ!")
        self._print_summary()
    
    def _load_approved_keywords(self) -> Set[str]:
        """ìŠ¹ì¸ëœ í‚¤ì›Œë“œ ë¡œë“œ"""
        approved_path = 'data/fine_tuning/keywords/approved_keywords.json'
        fallback_path = 'data/fine_tuning/keywords/extracted_keywords.json'
        
        # ìŠ¹ì¸ëœ í‚¤ì›Œë“œ íŒŒì¼ ìš°ì„  ì‹œë„
        for path in [approved_path, fallback_path]:
            if os.path.exists(path):
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        keywords = set(json.load(f))
                        print(f"âœ… í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ: {len(keywords)}ê°œ (ì¶œì²˜: {path})")
                        
                        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°
                        sample_keywords = list(keywords)[:10]
                        print(f"   ìƒ˜í”Œ í‚¤ì›Œë“œ: {', '.join(sample_keywords)}")
                        
                        return keywords
                except json.JSONDecodeError:
                    print(f"âŒ í‚¤ì›Œë“œ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {path}")
                    continue
        
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ íŒŒì¼ ì¤‘ í•˜ë‚˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:")
        print(f"  - {approved_path}")
        print(f"  - {fallback_path}")
        return set()
    
    def _generate_questions(self, keywords: Set[str]) -> List[str]:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        
        :param keywords: í‚¤ì›Œë“œ ì„¸íŠ¸
        :return: ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        print("\n2ï¸âƒ£ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        
        # ì´ë¯¸ ìƒì„±ëœ ì§ˆë¬¸ ë¡œë“œ (ì¶”ê°€ ë°©ì‹)
        existing_questions = []
        raw_questions_file = os.path.join(self.base_dir, 'raw_questions.json')
        
        if os.path.exists(raw_questions_file):
            try:
                with open(raw_questions_file, 'r', encoding='utf-8') as f:
                    existing_questions = json.load(f)
                    print(f"âœ… ê¸°ì¡´ ì§ˆë¬¸ {len(existing_questions)}ê°œ ë¡œë“œ ì™„ë£Œ")
            except json.JSONDecodeError:
                print("âš ï¸ ê¸°ì¡´ ì§ˆë¬¸ íŒŒì¼ ì†ìƒ. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ìƒì„±í•  ì§ˆë¬¸ ìˆ˜ ê²°ì • (ê¸°ì¡´ ì§ˆë¬¸ì´ ì¶©ë¶„í•˜ë©´ ì ê²Œ ìƒì„±)
        if len(existing_questions) >= 200:  # ìµœëŒ€ ì§ˆë¬¸ ìˆ˜
            print(f"âš ï¸ ì´ë¯¸ ì¶©ë¶„í•œ ì§ˆë¬¸({len(existing_questions)}ê°œ)ì´ ìˆìŠµë‹ˆë‹¤.")
            return existing_questions
            
        # í•„ìš”í•œ ì¶”ê°€ ì§ˆë¬¸ ìˆ˜ ê³„ì‚°
        target_total = 100  # ëª©í‘œ ì§ˆë¬¸ ìˆ˜
        num_to_generate = max(10, target_total - len(existing_questions))  # ìµœì†Œ 10ê°œëŠ” ìƒì„±
        
        print(f"ğŸ² {num_to_generate}ê°œì˜ ìƒˆ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        
        # ì§ˆë¬¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = QuestionGenerator(keywords)
        
        # ì§ˆë¬¸ ìƒì„±
        questions = generator.generate_questions(num_questions=num_to_generate)
        
        # ì§ˆë¬¸ í•„í„°ë§
        filtered_questions = generator.filter_questions(questions)
        
        # ì§ˆë¬¸ ì €ì¥ (ê¸°ì¡´ + ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤)
        all_questions = existing_questions + [q for q in filtered_questions if q not in existing_questions]
        
        # ì €ì¥
        save_path = os.path.join(self.base_dir, 'raw_questions.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(all_questions, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì§ˆë¬¸ ì €ì¥ ì™„ë£Œ: {len(filtered_questions)}ê°œ ì¶”ê°€, ì´ {len(all_questions)}ê°œ")
        
        return all_questions
    
    def _evaluate_and_improve_questions(self, questions: List[str]) -> List[Dict]:
        """ì§ˆë¬¸ í‰ê°€ ë° ê°œì„ """
        if not questions:
            print("í‰ê°€í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        improved_questions = []
        
        print(f"\nì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì„ í‰ê°€í•©ë‹ˆë‹¤.")
        print("ê° ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€í•˜ê³  í•„ìš”ì‹œ ê°œì„ í•´ì£¼ì„¸ìš”.\n")
        
        # í‰ê°€í•  ì§ˆë¬¸ ë²”ìœ„ ì„ íƒ
        try:
            start_idx = int(input(f"ì‹œì‘ ë²ˆí˜¸ (1-{len(questions)}, ê¸°ë³¸ 1): ") or 1) - 1
            end_idx = int(input(f"ë ë²ˆí˜¸ (1-{len(questions)}, ê¸°ë³¸ {min(30, len(questions))}): ") or min(30, len(questions)))
            
            # ë²”ìœ„ ê²€ì¦
            start_idx = max(0, start_idx)
            end_idx = min(len(questions), end_idx)
            
        except ValueError:
            start_idx = 0
            end_idx = min(30, len(questions))
        
        selected_questions = questions[start_idx:end_idx]
        print(f"\nğŸ“ {len(selected_questions)}ê°œ ì§ˆë¬¸ì„ í‰ê°€í•©ë‹ˆë‹¤ ({start_idx+1}ë²ˆë¶€í„° {end_idx}ë²ˆê¹Œì§€)")
        
        for idx, question in enumerate(selected_questions, start_idx + 1):
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {idx}: {question}")
            print('='*60)
            
            # í‰ê°€ ìˆ˜ì§‘
            evaluation = self._collect_evaluation(question, idx)
            
            # ê°œì„ ëœ ì§ˆë¬¸ ì²˜ë¦¬
            if evaluation['status'] == 'improved':
                improved_questions.append({
                    'original': question,
                    'improved': evaluation['improved_question'],
                    'scores': evaluation['scores'],
                    'feedback': evaluation['feedback']
                })
            elif evaluation['status'] == 'accepted':
                improved_questions.append({
                    'original': question,
                    'improved': question,  # ì›ë³¸ ê·¸ëŒ€ë¡œ
                    'scores': evaluation['scores'],
                    'feedback': evaluation['feedback']
                })
            # 'rejected'ì¸ ê²½ìš°ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            self.session_data['evaluations'].append(evaluation)
            
            # ê³„ì† ì§„í–‰ ì—¬ë¶€ (ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°)
            if idx < start_idx + len(selected_questions):
                cont = input("\në‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ y): ").lower()
                if cont == 'n':
                    print("í‰ê°€ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
        
        return improved_questions
    
    def _collect_evaluation(self, question: str, question_id: int) -> Dict:
        """ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ í‰ê°€ ìˆ˜ì§‘"""
        print("\nğŸ“Š ì§ˆë¬¸ í‰ê°€:")
        
        scores = {}
        
        # ê° ê¸°ì¤€ë³„ ì ìˆ˜ ì…ë ¥
        for criterion in self.evaluation_criteria:
            print(f"\n{criterion['name']} - {criterion['description']}")
            while True:
                try:
                    score_input = input(f"ì ìˆ˜ (0-10, ê¸°ë³¸ 7): ").strip()
                    score = int(score_input) if score_input else 7
                    
                    if 0 <= score <= 10:
                        scores[criterion['key']] = score
                        break
                    else:
                        print("0-10 ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                except ValueError:
                    print("ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = sum(scores.values()) / len(scores)
        
        print(f"\ní‰ê·  ì ìˆ˜: {avg_score:.1f}/10")
        
        # ì²˜ë¦¬ ë°©ë²• ì„ íƒ
        print("\nì²˜ë¦¬ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ìœ ì§€ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)")
        print("2. ê°œì„  (ìˆ˜ì •í•´ì„œ ì‚¬ìš©)")
        print("3. ì œì™¸ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)")
        
        while True:
            action = input("ì„ íƒ (1/2/3, ê¸°ë³¸ 1): ").strip()
            if action in ['1', '2', '3', '']:
                action = action or '1'
                break
            print("1, 2, 3 ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        
        # í”¼ë“œë°± ìˆ˜ì§‘
        feedback = input("\ní”¼ë“œë°± (ì„ íƒì‚¬í•­): ").strip()
        
        evaluation = {
            'question_id': question_id,
            'question': question,
            'scores': scores,
            'avg_score': avg_score,
            'feedback': feedback,
            'timestamp': datetime.now().isoformat()
        }
        
        # ì²˜ë¦¬ ë°©ë²•ì— ë”°ë¥¸ ì¶”ê°€ ì‘ì—…
        if action == '2':  # ê°œì„ 
            improved = input("ê°œì„ ëœ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if improved:
                evaluation['improved_question'] = improved
                evaluation['status'] = 'improved'
            else:
                evaluation['status'] = 'accepted'  # ê°œì„ ì•ˆ ì—†ìœ¼ë©´ ìœ ì§€
        elif action == '3':  # ì œì™¸
            evaluation['status'] = 'rejected'
        else:  # ìœ ì§€
            evaluation['status'] = 'accepted'
        
        return evaluation
    
    def _save_results(self, improved_questions: List[Dict]):
        """ê²°ê³¼ ì €ì¥"""
        if not improved_questions:
            print("âš ï¸ ì €ì¥í•  ê°œì„ ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 1. ê°œì„ ëœ ì§ˆë¬¸ë§Œ ì¶”ì¶œ
        refined_questions = [q['improved'] for q in improved_questions]
        
        # 2. ìµœì¢… ì§ˆë¬¸ ì €ì¥
        output_path = os.path.join(self.base_dir, 'refined_questions.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(refined_questions, f, ensure_ascii=False, indent=2)
        
        # 3. ì „ì²´ í‰ê°€ ë°ì´í„° ì €ì¥
        self.session_data['end_time'] = datetime.now().isoformat()
        self.session_data['improved_questions'] = improved_questions
        
        evaluation_path = os.path.join(self.base_dir, 'question_evaluations.json')
        with open(evaluation_path, 'w', encoding='utf-8') as f:
            json.dump(self.session_data, f, ensure_ascii=False, indent=2)
        
        # 4. ê°œì„  ì´ë ¥ ì €ì¥
        improvement_path = os.path.join(self.base_dir, 'improvement_history.json')
        with open(improvement_path, 'w', encoding='utf-8') as f:
            json.dump(improved_questions, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(refined_questions)}ê°œì˜ ê°œì„ ëœ ì§ˆë¬¸ ì €ì¥ ì™„ë£Œ")
        print(f"   - ìµœì¢… ì§ˆë¬¸: {output_path}")
        print(f"   - í‰ê°€ ë°ì´í„°: {evaluation_path}")
        print(f"   - ê°œì„  ì´ë ¥: {improvement_path}")
    
    def _print_summary(self):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        evaluations = self.session_data['evaluations']
        
        if not evaluations:
            print("í‰ê°€ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š Phase 1 ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        # ê¸°ë³¸ í†µê³„
        total_evaluated = len(evaluations)
        accepted = len([e for e in evaluations if e['status'] == 'accepted'])
        improved = len([e for e in evaluations if e['status'] == 'improved'])
        rejected = len([e for e in evaluations if e['status'] == 'rejected'])
        
        print(f"ì´ í‰ê°€ ì§ˆë¬¸: {total_evaluated}ê°œ")
        print(f"  - ìœ ì§€: {accepted}ê°œ ({accepted/total_evaluated*100:.1f}%)")
        print(f"  - ê°œì„ : {improved}ê°œ ({improved/total_evaluated*100:.1f}%)")
        print(f"  - ì œì™¸: {rejected}ê°œ ({rejected/total_evaluated*100:.1f}%)")
        
        # í‰ê·  ì ìˆ˜
        print(f"\nê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜:")
        for criterion in self.evaluation_criteria:
            key = criterion['key']
            scores = [e['scores'][key] for e in evaluations if key in e['scores']]
            if scores:
                avg = sum(scores) / len(scores)
                print(f"  - {criterion['name']}: {avg:.1f}/10")
        
        # ì „ì²´ í‰ê· 
        all_avg_scores = [e['avg_score'] for e in evaluations]
        if all_avg_scores:
            total_avg = sum(all_avg_scores) / len(all_avg_scores)
            print(f"\nì „ì²´ í‰ê·  ì ìˆ˜: {total_avg:.1f}/10")
        
        # ìµœì¢… ì§ˆë¬¸ ìˆ˜
        final_count = accepted + improved
        print(f"\nâœ¨ ìµœì¢… ì‚¬ìš© ê°€ëŠ¥í•œ ì§ˆë¬¸: {final_count}ê°œ")
        
        if final_count > 0:
            print("\në‹¤ìŒ ë‹¨ê³„: model_trainer - ë‹µë³€ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”.")
            print("python /src/fine_tuning/model_trainer.py")

def main():
    phase1 = Phase1QuestionImprovement()
    phase1.run_phase1()

if __name__ == "__main__":
    main()